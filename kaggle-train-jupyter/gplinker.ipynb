{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nGlobalPointer参考: https://github.com/gaohongkui/GlobalPointer_pytorch/blob/main/models/GlobalPointer.py\n稀疏多标签交叉熵损失参考: bert4keras源码\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport numpy as np\ndef sparse_multilabel_categorical_crossentropy(y_true=None, y_pred=None, mask_zero=False):\n    '''\n    稀疏多标签交叉熵损失的torch实现\n    '''\n    shape = y_pred.shape\n    y_true = y_true[..., 0] * shape[2] + y_true[..., 1]\n    y_pred = y_pred.reshape(shape[0], -1, np.prod(shape[2:]))\n    zeros = torch.zeros_like(y_pred[...,:1])\n    y_pred = torch.cat([y_pred, zeros], dim=-1)\n    if mask_zero:\n        infs = zeros + 1e12\n        y_pred = torch.cat([infs, y_pred[..., 1:]], dim=-1)\n    y_pos_2 = torch.gather(y_pred, index=y_true, dim=-1)\n    y_pos_1 = torch.cat([y_pos_2, zeros], dim=-1)\n    if mask_zero:\n        y_pred = torch.cat([-infs, y_pred[..., 1:]], dim=-1)\n        y_pos_2 = torch.gather(y_pred, index=y_true, dim=-1)\n    pos_loss = torch.logsumexp(-y_pos_1, dim=-1)\n    all_loss = torch.logsumexp(y_pred, dim=-1)\n    aux_loss = torch.logsumexp(y_pos_2, dim=-1) - all_loss\n    aux_loss = torch.clip(1 - torch.exp(aux_loss), 1e-10, 1)\n    neg_loss = all_loss + torch.log(aux_loss)\n    loss = torch.mean(torch.sum(pos_loss + neg_loss))\n    return loss\n\nclass RawGlobalPointer(nn.Module):\n    def __init__(self, hiddensize, ent_type_size, inner_dim, RoPE=True, tril_mask=True):\n        '''\n        :param encoder: BERT\n        :param ent_type_size: 实体数目\n        :param inner_dim: 64\n        '''\n        super().__init__()\n        self.ent_type_size = ent_type_size\n        self.inner_dim = inner_dim\n        self.hidden_size = hiddensize\n        self.dense = nn.Linear(self.hidden_size, self.ent_type_size * self.inner_dim * 2)\n\n        self.RoPE = RoPE\n        self.trail_mask = tril_mask\n\n    def sinusoidal_position_embedding(self, batch_size, seq_len, output_dim):\n        position_ids = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(-1)\n\n        indices = torch.arange(0, output_dim // 2, dtype=torch.float)\n        indices = torch.pow(10000, -2 * indices / output_dim)\n        embeddings = position_ids * indices\n        embeddings = torch.stack([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n        embeddings = embeddings.repeat((batch_size, *([1] * len(embeddings.shape))))\n        embeddings = torch.reshape(embeddings, (batch_size, seq_len, output_dim))\n        embeddings = embeddings.to(self.device)\n        return embeddings\n\n    def forward(self, context_outputs,  attention_mask):\n        self.device = attention_mask.device\n        last_hidden_state = context_outputs[0]\n        batch_size = last_hidden_state.size()[0]\n        seq_len = last_hidden_state.size()[1]\n        outputs = self.dense(last_hidden_state)\n        outputs = torch.split(outputs, self.inner_dim * 2, dim=-1)\n        outputs = torch.stack(outputs, dim=-2)\n        qw, kw = outputs[..., :self.inner_dim], outputs[..., self.inner_dim:]\n        if self.RoPE:\n            # pos_emb:(batch_size, seq_len, inner_dim)\n            pos_emb = self.sinusoidal_position_embedding(batch_size, seq_len, self.inner_dim)\n            cos_pos = pos_emb[..., None, 1::2].repeat_interleave(2, dim=-1)\n            sin_pos = pos_emb[..., None, ::2].repeat_interleave(2, dim=-1)\n            qw2 = torch.stack([-qw[..., 1::2], qw[..., ::2]], -1)\n            qw2 = qw2.reshape(qw.shape)\n            qw = qw * cos_pos + qw2 * sin_pos\n            kw2 = torch.stack([-kw[..., 1::2], kw[..., ::2]], -1)\n            kw2 = kw2.reshape(kw.shape)\n            kw = kw * cos_pos + kw2 * sin_pos\n        # logits:(batch_size, ent_type_size, seq_len, seq_len)\n        logits = torch.einsum('bmhd,bnhd->bhmn', qw, kw)\n        # padding mask\n        pad_mask = attention_mask.unsqueeze(1).unsqueeze(1).expand(batch_size, self.ent_type_size, seq_len, seq_len)\n        logits = logits * pad_mask - (1 - pad_mask) * 1e12\n        # 排除下三角\n        if self.trail_mask:\n            mask = torch.tril(torch.ones_like(logits), -1)\n            logits = logits - mask * 1e12\n\n        return logits / self.inner_dim ** 0.5","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"PyTorch optimization for BERT model.\"\"\"\n\nimport math\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.optim.optimizer import required\nfrom torch.nn.utils import clip_grad_norm_\nimport logging\nimport abc\nimport sys\n\nlogger = logging.getLogger(__name__)\n\n\nif sys.version_info >= (3, 4):\n    ABC = abc.ABC\nelse:\n    ABC = abc.ABCMeta('ABC', (), {})\n\n\nclass _LRSchedule(ABC):\n    \"\"\" Parent of all LRSchedules here. \"\"\"\n    warn_t_total = False        # is set to True for schedules where progressing beyond t_total steps doesn't make sense\n    def __init__(self, warmup=0.002, t_total=-1, **kw):\n        \"\"\"\n        :param warmup:  what fraction of t_total steps will be used for linear warmup\n        :param t_total: how many training steps (updates) are planned\n        :param kw:\n        \"\"\"\n        super(_LRSchedule, self).__init__(**kw)\n        if t_total < 0:\n            logger.warning(\"t_total value of {} results in schedule not being applied\".format(t_total))\n        if not 0.0 <= warmup < 1.0 and not warmup == -1:\n            raise ValueError(\"Invalid warmup: {} - should be in [0.0, 1.0[ or -1\".format(warmup))\n        warmup = max(warmup, 0.)\n        self.warmup, self.t_total = float(warmup), float(t_total)\n        self.warned_for_t_total_at_progress = -1\n\n    def get_lr(self, step, nowarn=False):\n        \"\"\"\n        :param step:    which of t_total steps we're on\n        :param nowarn:  set to True to suppress warning regarding training beyond specified 't_total' steps\n        :return:        learning rate multiplier for current update\n        \"\"\"\n        if self.t_total < 0:\n            return 1.\n        progress = float(step) / self.t_total\n        ret = self.get_lr_(progress)\n        # warning for exceeding t_total (only active with warmup_linear\n        if not nowarn and self.warn_t_total and progress > 1. and progress > self.warned_for_t_total_at_progress:\n            logger.warning(\n                \"Training beyond specified 't_total'. Learning rate multiplier set to {}. Please set 't_total' of {} correctly.\"\n                    .format(ret, self.__class__.__name__))\n            self.warned_for_t_total_at_progress = progress\n        # end warning\n        return ret\n\n    @abc.abstractmethod\n    def get_lr_(self, progress):\n        \"\"\"\n        :param progress:    value between 0 and 1 (unless going beyond t_total steps) specifying training progress\n        :return:            learning rate multiplier for current update\n        \"\"\"\n        return 1.\n\n\nclass ConstantLR(_LRSchedule):\n    def get_lr_(self, progress):\n        return 1.\n\n\nclass WarmupCosineSchedule(_LRSchedule):\n    \"\"\"\n    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n    Decreases learning rate from 1. to 0. over remaining `1 - warmup` steps following a cosine curve.\n    If `cycles` (default=0.5) is different from default, learning rate follows cosine function after warmup.\n    \"\"\"\n    warn_t_total = True\n    def __init__(self, warmup=0.002, t_total=-1, cycles=.5, **kw):\n        \"\"\"\n        :param warmup:      see LRSchedule\n        :param t_total:     see LRSchedule\n        :param cycles:      number of cycles. Default: 0.5, corresponding to cosine decay from 1. at progress==warmup and 0 at progress==1.\n        :param kw:\n        \"\"\"\n        super(WarmupCosineSchedule, self).__init__(warmup=warmup, t_total=t_total, **kw)\n        self.cycles = cycles\n\n    def get_lr_(self, progress):\n        if progress < self.warmup:\n            return progress / self.warmup\n        else:\n            progress = (progress - self.warmup) / (1 - self.warmup)   # progress after warmup\n            return 0.5 * (1. + math.cos(math.pi * self.cycles * 2 * progress))\n\n\nclass WarmupCosineWithHardRestartsSchedule(WarmupCosineSchedule):\n    \"\"\"\n    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n    If `cycles` (default=1.) is different from default, learning rate follows `cycles` times a cosine decaying\n    learning rate (with hard restarts).\n    \"\"\"\n    def __init__(self, warmup=0.002, t_total=-1, cycles=1., **kw):\n        super(WarmupCosineWithHardRestartsSchedule, self).__init__(warmup=warmup, t_total=t_total, cycles=cycles, **kw)\n        assert(cycles >= 1.)\n\n    def get_lr_(self, progress):\n        if progress < self.warmup:\n            return progress / self.warmup\n        else:\n            progress = (progress - self.warmup) / (1 - self.warmup)     # progress after warmup\n            ret = 0.5 * (1. + math.cos(math.pi * ((self.cycles * progress) % 1)))\n            return ret\n\n\nclass WarmupCosineWithWarmupRestartsSchedule(WarmupCosineWithHardRestartsSchedule):\n    \"\"\"\n    All training progress is divided in `cycles` (default=1.) parts of equal length.\n    Every part follows a schedule with the first `warmup` fraction of the training steps linearly increasing from 0. to 1.,\n    followed by a learning rate decreasing from 1. to 0. following a cosine curve.\n    \"\"\"\n    def __init__(self, warmup=0.002, t_total=-1, cycles=1., **kw):\n        assert(warmup * cycles < 1.)\n        warmup = warmup * cycles if warmup >= 0 else warmup\n        super(WarmupCosineWithWarmupRestartsSchedule, self).__init__(warmup=warmup, t_total=t_total, cycles=cycles, **kw)\n\n    def get_lr_(self, progress):\n        progress = progress * self.cycles % 1.\n        if progress < self.warmup:\n            return progress / self.warmup\n        else:\n            progress = (progress - self.warmup) / (1 - self.warmup)     # progress after warmup\n            ret = 0.5 * (1. + math.cos(math.pi * progress))\n            return ret\n\n\nclass WarmupConstantSchedule(_LRSchedule):\n    \"\"\"\n    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n    Keeps learning rate equal to 1. after warmup.\n    \"\"\"\n    def get_lr_(self, progress):\n        if progress < self.warmup:\n            return progress / self.warmup\n        return 1.\n\n\nclass WarmupLinearSchedule(_LRSchedule):\n    \"\"\"\n    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n    Linearly decreases learning rate from 1. to 0. over remaining `1 - warmup` steps.\n    \"\"\"\n    warn_t_total = True\n    def get_lr_(self, progress):\n        if progress < self.warmup:\n            return progress / self.warmup\n        return max((progress - 1.) / (self.warmup - 1.), 0.)\n\n\nSCHEDULES = {\n    None:       ConstantLR,\n    \"none\":     ConstantLR,\n    \"warmup_cosine\": WarmupCosineSchedule,\n    \"warmup_constant\": WarmupConstantSchedule,\n    \"warmup_linear\": WarmupLinearSchedule\n}\n\nclass EMA():\n    def __init__(self, model, decay):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n\n    def register(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                assert name in self.shadow\n                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n                self.shadow[name] = new_average.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                assert name in self.shadow\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                assert name in self.backup\n                param.data = self.backup[name]\n        self.backup = {}\n\nclass BertAdam(Optimizer):\n    \"\"\"Implements BERT version of Adam algorithm with weight decay fix.\n    Params:\n        lr: learning rate\n        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1\n        t_total: total number of training steps for the learning\n            rate schedule, -1  means constant learning rate of 1. (no warmup regardless of warmup setting). Default: -1\n        schedule: schedule to use for the warmup (see above).\n            Can be `'warmup_linear'`, `'warmup_constant'`, `'warmup_cosine'`, `'none'`, `None` or a `_LRSchedule` object (see below).\n            If `None` or `'none'`, learning rate is always kept constant.\n            Default : `'warmup_linear'`\n        b1: Adams b1. Default: 0.9\n        b2: Adams b2. Default: 0.999\n        e: Adams epsilon. Default: 1e-6\n        weight_decay: Weight decay. Default: 0.01\n        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0\n    \"\"\"\n    def __init__(self, params, lr=required, warmup=-1, t_total=-1, schedule='warmup_linear',\n                 b1=0.9, b2=0.999, e=1e-6, weight_decay=0.01, max_grad_norm=1.0, **kwargs):\n        if lr is not required and lr < 0.0:\n            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n        if not isinstance(schedule, _LRSchedule) and schedule not in SCHEDULES:\n            raise ValueError(\"Invalid schedule parameter: {}\".format(schedule))\n        if not 0.0 <= b1 < 1.0:\n            raise ValueError(\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\".format(b1))\n        if not 0.0 <= b2 < 1.0:\n            raise ValueError(\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\".format(b2))\n        if not e >= 0.0:\n            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(e))\n        # initialize schedule object\n        if not isinstance(schedule, _LRSchedule):\n            schedule_type = SCHEDULES[schedule]\n            schedule = schedule_type(warmup=warmup, t_total=t_total)\n        else:\n            if warmup != -1 or t_total != -1:\n                logger.warning(\"warmup and t_total on the optimizer are ineffective when _LRSchedule object is provided as schedule. \"\n                               \"Please specify custom warmup and t_total in _LRSchedule object.\")\n        defaults = dict(lr=lr, schedule=schedule,\n                        b1=b1, b2=b2, e=e, weight_decay=weight_decay,\n                        max_grad_norm=max_grad_norm)\n        super(BertAdam, self).__init__(params, defaults)\n\n    def get_lr(self):\n        lr = []\n        for group in self.param_groups:\n            for p in group['params']:\n                state = self.state[p]\n                if len(state) == 0:\n                    return [0]\n                lr_scheduled = group['lr']\n                lr_scheduled *= group['schedule'].get_lr(state['step'])\n                lr.append(lr_scheduled)\n        return lr\n\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['next_m'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['next_v'] = torch.zeros_like(p.data)\n\n                next_m, next_v = state['next_m'], state['next_v']\n                beta1, beta2 = group['b1'], group['b2']\n\n                # Add grad clipping\n                if group['max_grad_norm'] > 0:\n                    clip_grad_norm_(p, group['max_grad_norm'])\n\n                # Decay the first and second moment running average coefficient\n                # In-place operations to update the averages at the same time\n                next_m.mul_(beta1).add_(1 - beta1, grad)\n                next_v.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                update = next_m / (next_v.sqrt() + group['e'])\n\n                # Just adding the square of the weights to the loss function is *not*\n                # the correct way of using L2 regularization/weight decay with Adam,\n                # since that will interact with the m and v parameters in strange ways.\n                #\n                # Instead we want to decay the weights in a manner that doesn't interact\n                # with the m/v parameters. This is equivalent to adding the square\n                # of the weights to the loss with plain (non-momentum) SGD.\n                if group['weight_decay'] > 0.0:\n                    update += group['weight_decay'] * p.data\n\n                lr_scheduled = group['lr']\n                lr_scheduled *= group['schedule'].get_lr(state['step'])\n\n                update_with_lr = lr_scheduled * update\n                p.data.add_(-update_with_lr)\n\n                state['step'] += 1\n\n                # step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n                # No bias correction\n                # bias_correction1 = 1 - beta1 ** state['step']\n                # bias_correction2 = 1 - beta2 ** state['step']\n\n        return loss\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import  pandas as pd\n\n\n# def process_data(file_path):\n#     data = pd.read_csv(file_path)\n#     # print(data.columns)\n#     processed_data = []\n#\n#     for index, line in data.iterrows():\n#         dct = {\n#             \"text\": [],\n#             \"spo_list\": {\n#                 \"subject\": [],\n#                 \"object\": [],\n#                 \"predicate\": [],\n#             },\n#         }\n#         dct[\"text\"] = line[\"text\"]\n#         dct[\"spo_list\"][\"object\"] = line[\"effect\"]\n#         dct[\"spo_list\"][\"subject\"] = line[\"drug\"]\n#         dct[\"spo_list\"][\"predicate\"] = \"causes\"\n#         processed_data.append(dct)\n#\n#\n#     return processed_data\n#\n#\n#\ndef merge_data(data):\n    name_list = []\n    data_list = []\n\n    for each in data:\n        if each[\"text\"] not in name_list:\n            name_list.append(each[\"text\"])\n            data_list.append(each)\n        else:\n            index = name_list.index(each[\"text\"])\n            data_list[index][\"spo_list\"].append(each[\"spo_list\"][0])\n\n    return data_list\n\n\ndef process_data(file_path):\n    data = pd.read_csv(file_path)\n    # print(data.columns)\n    processed_data = []\n\n    for index, line in data.iterrows():\n        dct = {\n            \"text\": [],\n            \"spo_list\": [],\n        }\n        spo = {\n            \"subject\": \"\",\n            \"predicate\": \"\",\n            \"object\": \"\",\n            \"subject_type\":\"drug\",\n            \"object_type\":\"adverse\"\n\n        }\n\n        dct[\"text\"] = line[\"text\"]\n        spo[\"object\"] = line[\"effect\"]\n        spo[\"subject\"] = line[\"drug\"]\n        spo[\"predicate\"] = \"causes\"\n        dct[\"spo_list\"].append((spo[\"subject\"],spo[\"predicate\"],spo[\"object\"],spo[\"subject_type\"],spo[\"object_type\"]))\n        processed_data.append(dct)\n\n    return merge_data(processed_data)\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\n@Auth: Xhw\n@Description: CHIP/CBLUE 医学实体关系抽取，数据来源 https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414\n\"\"\"\nimport json\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\n\ndef load_name(filename):\n    print(filename)\n    return process_data(filename)\n    #{\"text\": \"产后抑郁症@区分产后抑郁症与轻度情绪失调（产后忧郁或“婴儿忧郁”）是重要的，因为轻度情绪失调不需要治疗。\", \"spo_list\": [{\"Combined\": false, \"predicate\": \"鉴别诊断\", \"subject\": \"产后抑郁症\", \"subject_type\": \"疾病\", \"object\": {\"@value\": \"轻度情绪失调\"}, \"object_type\": {\"@value\": \"疾病\"}}]}\n\n\ndef sequence_padding(inputs, length=None, value=0, seq_dims=1, mode='post'):\n    \"\"\"Numpy函数，将序列padding到同一长度\n    \"\"\"\n    if length is None:\n        length = np.max([np.shape(x)[:seq_dims] for x in inputs], axis=0)\n    elif not hasattr(length, '__getitem__'):\n        length = [length]\n\n    slices = [np.s_[:length[i]] for i in range(seq_dims)]\n    slices = tuple(slices) if len(slices) > 1 else slices[0]\n    pad_width = [(0, 0) for _ in np.shape(inputs[0])]\n\n    outputs = []\n    for x in inputs:\n        x = x[slices]\n        for i in range(seq_dims):\n            if mode == 'post':\n                pad_width[i] = (0, length[i] - np.shape(x)[i])\n            elif mode == 'pre':\n                pad_width[i] = (length[i] - np.shape(x)[i], 0)\n            else:\n                raise ValueError('\"mode\" argument must be \"post\" or \"pre\".')\n        x = np.pad(x, pad_width, 'constant', constant_values=value)\n        outputs.append(x)\n    return np.array(outputs)\n\ndef search(pattern, sequence):\n    \"\"\"从sequence中寻找子串pattern\n    如果找到，返回第一个下标；否则返回-1。\n    \"\"\"\n    n = len(pattern)\n    for i in range(len(sequence)):\n        if sequence[i:i + n] == pattern:\n            return i\n    return -1\n\nclass data_generator(Dataset):\n    def __init__(self, data, tokenizer, max_len, schema):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.schema = schema #spo\n    def __len__(self):\n        return len(self.data)\n\n    def encoder(self, item):\n        text = item[\"text\"]\n        encoder_text = self.tokenizer(text, return_offsets_mapping=True, max_length=self.max_len, truncation=True)\n        input_ids = encoder_text[\"input_ids\"]\n        token_type_ids = encoder_text[\"token_type_ids\"] #RoBERTa不需要NSP任务\n        attention_mask = encoder_text[\"attention_mask\"]\n        spoes = set()\n        for s, p, o, s_t, o_t in item[\"spo_list\"]:\n            s = self.tokenizer.encode(s, add_special_tokens=False)\n            p = self.schema[s_t + \"_\" + p + \"_\" +o_t]\n            o = self.tokenizer.encode(o, add_special_tokens=False)\n            sh = search(s, input_ids)\n            oh = search(o, input_ids)\n            if sh != -1 and oh != -1:\n                spoes.add((sh, sh+len(s)-1, p, oh, oh+len(o)-1))\n        entity_labels = [set() for i in range(2)]\n        head_labels = [set() for i in range(len(self.schema))]\n        tail_labels = [set() for i in range(len(self.schema))]\n        for sh, st, p, oh, ot in spoes:\n            entity_labels[0].add((sh, st)) #实体提取：2个类型，头实体or尾实体\n            entity_labels[1].add((oh, ot))\n            head_labels[p].add((sh, oh)) #类似TP-Linker\n            tail_labels[p].add((st, ot))\n        for label in entity_labels+head_labels+tail_labels:\n            if not label:\n                label.add((0,0))\n        # 例如entity = [{(1,3)}, {(4,5), (7,9)}]\n        # entity[0]即{(1,3)}代表头实体首尾， entity[1]即{(4,5),{7,9}}代表尾实体首尾\n        # 需要标签对齐为 [[[1,3][0,0]] , [[4,5][7,9]]]\n        entity_labels = sequence_padding([list(l) for l in entity_labels])\n        head_labels = sequence_padding([list(l) for l in head_labels])\n        tail_labels = sequence_padding([list(l) for l in tail_labels])\n        return text, entity_labels, head_labels, tail_labels, input_ids, attention_mask, token_type_ids\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        return self.encoder(item)\n\n    @staticmethod\n    def collate(examples):\n        batch_token_ids, batch_mask_ids, batch_token_type_ids = [], [], []\n        batch_entity_labels, batch_head_labels, batch_tail_labels = [], [], []\n        text_list = []\n        for item in examples:\n            text, entity_labels, head_labels, tail_labels, input_ids, attention_mask, token_type_ids = item\n            batch_entity_labels.append(entity_labels)\n            batch_head_labels.append(head_labels)\n            batch_tail_labels.append(tail_labels)\n            batch_token_ids.append(input_ids)\n            batch_mask_ids.append(attention_mask)\n            batch_token_type_ids.append(token_type_ids)\n            text_list.append(text)\n\n        batch_token_ids = torch.tensor(sequence_padding(batch_token_ids)).long()\n        batch_mask_ids = torch.tensor(sequence_padding(batch_mask_ids)).float()\n        batch_token_type_ids = torch.tensor(sequence_padding(batch_token_type_ids)).long()#RoBERTa 不需要NSP\n        batch_entity_labels = torch.tensor(sequence_padding(batch_entity_labels, seq_dims=2)).long()\n        batch_head_labels = torch.tensor(sequence_padding(batch_head_labels, seq_dims=2)).long()\n        batch_tail_labels = torch.tensor(sequence_padding(batch_tail_labels, seq_dims=2)).long()\\\n\n        return text_list, batch_token_ids, batch_mask_ids, batch_token_type_ids, batch_entity_labels, batch_head_labels, batch_tail_labels\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\n@Auth: Xhw\n@Description: token-pair范式的实体关系抽取pytorch实现\n\"\"\"\nimport torch\nimport json\nimport sys\nimport numpy as np\nimport torch.nn as nn\nfrom transformers import BertTokenizerFast, BertModel,AutoTokenizer, AutoModelForMaskedLM\nfrom torch.utils.data import DataLoader\nimport configparser\nfrom torch.utils.tensorboard import SummaryWriter\n\n\n\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n\n\ncon = configparser.ConfigParser()\ncon.read('/kaggle/input/gplinkerdata/config.ini', encoding='utf8')\nargs_path = dict(dict(con.items('paths')), **dict(con.items(\"para\")))\ntokenizer = BertTokenizerFast.from_pretrained(args_path[\"model_path\"], do_lower_case=True)\nencoder = BertModel.from_pretrained(args_path[\"model_path\"])\n\nwith open(args_path[\"schema_data\"], 'r', encoding='utf-8') as f:\n    schema = {}\n    for idx, item in enumerate(f):\n        item = json.loads(item.rstrip())\n        schema[item[\"subject_type\"]+\"_\"+item[\"predicate\"]+\"_\"+item[\"object_type\"]] = idx\nid2schema = {}\nfor k,v in schema.items(): id2schema[v]=k\ntrain_data = data_generator(load_name(args_path[\"train_file\"]), tokenizer, max_len=con.getint(\"para\", \"maxlen\"), schema=schema)\ndev_data = data_generator(load_name(args_path[\"val_file\"]), tokenizer, max_len=con.getint(\"para\", \"maxlen\"), schema=schema)\ntrain_loader = DataLoader(train_data , batch_size=con.getint(\"para\", \"batch_size\"), shuffle=True, collate_fn=train_data.collate)\ndev_loader = DataLoader(dev_data , batch_size=con.getint(\"para\", \"batch_size\"), shuffle=True, collate_fn=dev_data.collate)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmention_detect = RawGlobalPointer(hiddensize=768, ent_type_size=2, inner_dim=64).to(device)#实体关系抽取任务默认不提取实体类型\ns_o_head = RawGlobalPointer(hiddensize=768, ent_type_size=len(schema), inner_dim=64, RoPE=False, tril_mask=False).to(device)\ns_o_tail = RawGlobalPointer(hiddensize=768, ent_type_size=len(schema), inner_dim=64, RoPE=False, tril_mask=False).to(device)\nclass ERENet(nn.Module):\n    def __init__(self, encoder, a, b, c):\n        super(ERENet, self).__init__()\n        self.mention_detect = a\n        self.s_o_head = b\n        self.s_o_tail = c\n        self.encoder = encoder\n\n    def forward(self, batch_token_ids, batch_mask_ids, batch_token_type_ids):\n        outputs = self.encoder(batch_token_ids, batch_mask_ids, batch_token_type_ids)\n       \n\n        mention_outputs = self.mention_detect(outputs, batch_mask_ids)\n        so_head_outputs = self.s_o_head(outputs, batch_mask_ids)\n        so_tail_outputs = self.s_o_tail(outputs, batch_mask_ids)\n        return mention_outputs, so_head_outputs, so_tail_outputs\n\nnet = ERENet(encoder, mention_detect, s_o_head, s_o_tail).to(device)\n# optimizer = torch.optim.AdamW(\n# \tnet.parameters(),\n#     lr=1e-5\n# )\ndef set_optimizer(model, train_steps=None):\n    param_optimizer = list(model.named_parameters())\n    param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n         'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n    optimizer = BertAdam(optimizer_grouped_parameters,\n                         lr=2e-5,\n                         warmup=0.1,\n                         t_total=train_steps)\n    return optimizer\n\noptimizer = set_optimizer(net, train_steps= (int(len(train_data) / con.getint(\"para\", \"batch_size\")) + 1) * con.getint(\"para\", \"epochs\"))\ntotal_loss, total_f1 = 0., 0.\nfor eo in range(con.getint(\"para\", \"epochs\")):\n    for idx, batch in enumerate(train_loader):\n        text, batch_token_ids, batch_mask_ids, batch_token_type_ids, batch_entity_labels, batch_head_labels, batch_tail_labels = batch\n        batch_token_ids, batch_mask_ids, batch_token_type_ids, batch_entity_labels, batch_head_labels, batch_tail_labels = \\\n            batch_token_ids.to(device), batch_mask_ids.to(device), batch_token_type_ids.to(device), batch_entity_labels.to(device), batch_head_labels.to(device), batch_tail_labels.to(device)\n        logits1, logits2, logits3 = net(batch_token_ids, batch_mask_ids, batch_token_type_ids)\n        loss1 = sparse_multilabel_categorical_crossentropy(y_true=batch_entity_labels, y_pred=logits1, mask_zero=True)\n        loss2 = sparse_multilabel_categorical_crossentropy(y_true=batch_head_labels, y_pred=logits2, mask_zero=True)\n        loss3 = sparse_multilabel_categorical_crossentropy(y_true=batch_tail_labels, y_pred=logits3, mask_zero=True)\n        loss = sum([loss1, loss2, loss3]) / 3\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        sys.stdout.write(\"\\r [EPOCH %d/%d] [Loss:%f]\"%(eo, con.getint(\"para\", \"epochs\"), loss.item()))\n    if eo % 3 == 0:\n        torch.save(net.state_dict(), './erenet.pth')\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}