{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\n\n\nREL_SIZE = 2\n\nREL_PATH = \"/kaggle/input/casrel-adr-data/rel.csv\"\nTRAIN_PATH = '/kaggle/input/casrel-adr-data/adr-train.csv'\n#TEST_PATH = './data/input/adr-test.csv'\n\nBERT_MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.2\"#'./bert-model/biobert-base-cased-v1.2'\n\nMODEL_DIR = './'\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nBATCH_SIZE = 100  #作为demo batch设为2，在训练时调味100\nBERT_DIM = 768  #BERT的输出维数\nLR = 1e-3       #学习率\nEPOCH = 100\n\n# sub和obj的head与tail的判断阈值\nSUB_HEAD_BAR = 0.6\nSUB_TAIL_BAR = 0.6\n\nOBJ_HEAD_BAR = 0.6\nOBJ_TAIL_BAR = 0.6\n\n\n#降权\nCLS_WEIGHT_COEF = [0.3, 1.0]\nSUB_WEIGHT_COEF = 3\n\n\nEPS = 1e-10","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-17T10:41:01.664456Z","iopub.execute_input":"2023-04-17T10:41:01.665442Z","iopub.status.idle":"2023-04-17T10:41:01.672490Z","shell.execute_reply.started":"2023-04-17T10:41:01.665402Z","shell.execute_reply":"2023-04-17T10:41:01.671138Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import  pandas as pd\n\n\n# def process_data(file_path):\n#     data = pd.read_csv(file_path)\n#     # print(data.columns)\n#     processed_data = []\n#\n#     for index, line in data.iterrows():\n#         dct = {\n#             \"text\": [],\n#             \"spo_list\": {\n#                 \"subject\": [],\n#                 \"object\": [],\n#                 \"predicate\": [],\n#             },\n#         }\n#         dct[\"text\"] = line[\"text\"]\n#         dct[\"spo_list\"][\"object\"] = line[\"effect\"]\n#         dct[\"spo_list\"][\"subject\"] = line[\"drug\"]\n#         dct[\"spo_list\"][\"predicate\"] = \"causes\"\n#         processed_data.append(dct)\n#\n#\n#     return processed_data\n#\n#\n#\ndef merge_data(data):\n    name_list = []\n    data_list = []\n\n    for each in data:\n        if each[\"text\"] not in name_list:\n            name_list.append(each[\"text\"])\n            data_list.append(each)\n        else:\n            index = name_list.index(each[\"text\"])\n            data_list[index][\"spo_list\"].append(each[\"spo_list\"][0])\n\n    return data_list\n\n\ndef process_data(file_path):\n    data = pd.read_csv(file_path)\n    # print(data.columns)\n    processed_data = []\n\n    for index, line in data.iterrows():\n        dct = {\n            \"text\": [],\n            \"spo_list\": [],\n        }\n        spo = {\n            \"subject\": \"\",\n            \"object\": \"\",\n            \"predicate\": \"\"\n        }\n        dct[\"text\"] = line[\"text\"]\n\n        spo[\"object\"] = line[\"effect\"]\n        spo[\"subject\"] = line[\"drug\"]\n        spo[\"predicate\"] = \"causes\"\n\n        dct[\"spo_list\"].append(spo)\n        processed_data.append(dct)\n\n    return merge_data(processed_data)\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-17T10:41:01.727255Z","iopub.execute_input":"2023-04-17T10:41:01.728893Z","iopub.status.idle":"2023-04-17T10:41:01.738356Z","shell.execute_reply.started":"2023-04-17T10:41:01.728849Z","shell.execute_reply":"2023-04-17T10:41:01.737244Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch.utils.data as data\nimport pandas as pd\nimport random\nfrom transformers import BertTokenizerFast\n\n#返回一个rel2id 和id2rel\ndef get_rel():\n    df = pd.read_csv(REL_PATH, names=['rel', 'id'])\n    return df['rel'].tolist(), dict(df.values)\n\n#生成长度为len，hot_pos位置为1其余位置为0的独热编码\ndef multihot(length, hot_pos):\n    return [1 if i in hot_pos else 0 for i in range(length)]\n\n\n\nclass Dataset(data.Dataset):\n    def __init__(self, type='train'):\n        super().__init__()\n        _, self.rel2id = get_rel()\n\n        # 加载文件\n        if type == 'train':\n            file_path = TRAIN_PATH\n        elif type == 'test':\n            file_path = TEST_PATH\n\n        self.lines = process_data(file_path)\n\n        # 加载bert\n        self.tokenizer = BertTokenizerFast.from_pretrained(BERT_MODEL_NAME)\n\n    def __len__(self):\n        return len(self.lines)\n\n    def __getitem__(self, index):\n        info = self.lines[index]\n        # print(type(info))\n        # exit()\n        tokenized = self.tokenizer(info['text'], return_offsets_mapping=True)\n        info['input_ids'] = tokenized['input_ids']\n        info['offset_mapping'] = tokenized['offset_mapping']\n\n        return self.parse_dict(info)\n\n\n    def get_pos_id(self, source, elem):\n        for head_id in range(len(source)):\n            tail_id = head_id + len(elem)\n            if source[head_id:tail_id] == elem:\n                return head_id, tail_id - 1\n\n\n    def collate_fn(self,batch):\n        #获得最长的句子长度,便于填充pad\n        batch.sort(key=lambda x : len(x[\"input_ids\"]),reverse=True)\n        max_len = len(batch[0][\"input_ids\"])\n\n        batch_text = {\n            'text': [],\n            'input_ids': [],\n            'offset_mapping': [],\n            'triple_list': [],\n        }\n        batch_mask = []\n\n        batch_sub = {\n            'heads_seq': [],\n            'tails_seq': [],\n        }\n        batch_sub_rnd = {\n            'head_seq': [],\n            'tail_seq': [],\n        }\n        batch_obj_rel = {\n            'heads_mx': [],\n            'tails_mx': [],\n        }\n\n        #对batch中的每一个item进行处理\n        for item in batch:\n            input_ids = item[\"input_ids\"]  #对元素进行pad填充\n            item_len = len(input_ids)\n            pad_len = max_len - item_len\n            input_ids = input_ids + [0] * pad_len\n            mask = [1] * item_len + [0] * pad_len\n            # print(mask)\n            # exit()\n\n            sub_head_seq = multihot(max_len,item[\"sub_head_ids\"])\n            sub_tail_seq = multihot(max_len, item[\"sub_tail_ids\"])\n            # print(item[\"sub_head_ids\"])\n            # print(sub_head_seq)\n            # exit()\n\n            if len(item['triple_id_list']) == 0:  #如果没有三元组则continue\n                continue\n\n            sub_rnd = random.choice(item['triple_id_list'])[0]\n            sub_rnd_head_seq = multihot(max_len, [sub_rnd[0]])\n            sub_rnd_tail_seq = multihot(max_len, [sub_rnd[1]])\n\n            #根据随机subject计算relations矩阵\n            obj_head_mx = [[0] * REL_SIZE for _ in range(max_len)]   #生成两个二维全0矩阵（一个head矩阵一个tail矩阵）\n            obj_tail_mx = [[0] * REL_SIZE for _ in range(max_len)]\n\n            for triple in item[\"triple_id_list\"]:                    #对全0矩阵进行填充获得obj的head和tail的rel矩阵\n                rel_id = triple[1]\n                head_id, tail_id = triple[2]\n                if triple[0] == sub_rnd:       # 对于本课题可以取消这一步，因为数据集中有且仅有唯一的sub\n                    obj_head_mx[head_id][rel_id] = 1\n                    obj_tail_mx[tail_id][rel_id] = 1\n\n            #重新组装batch，一条item压入一组信息\n            batch_text[\"text\"].append(item[\"text\"])\n            batch_text[\"input_ids\"].append(input_ids)\n            batch_text[\"offset_mapping\"].append(item[\"offset_mapping\"])\n            batch_text[\"triple_list\"].append(item[\"triple_list\"])\n\n            batch_mask.append(mask)\n\n            batch_sub[\"heads_seq\"].append(sub_head_seq)\n            batch_sub[\"tails_seq\"].append(sub_tail_seq)\n\n            batch_sub_rnd[\"head_seq\"].append(sub_rnd_head_seq)\n            batch_sub_rnd[\"tail_seq\"].append(sub_rnd_tail_seq)\n\n            # print(sub_rnd_head_seq)\n            # print(sub_head_seq)\n            # exit()\n\n            batch_obj_rel[\"heads_mx\"].append(obj_head_mx)\n            batch_obj_rel[\"tails_mx\"].append(obj_tail_mx)\n\n        return batch_mask,(batch_text,batch_sub_rnd),(batch_sub,batch_obj_rel)\n\n\n\n\n    def parse_dict(self, info):  #对dict串进行解析\n        text = info['text']\n        input_ids = info['input_ids']  #整个text的input_ids\n\n        dct = {\n            'text': text,\n            'input_ids': input_ids,\n            'offset_mapping': info['offset_mapping'],\n            'sub_head_ids': [],\n            'sub_tail_ids': [],\n            'triple_list': [],\n            'triple_id_list': []\n        }\n\n        for spo in info['spo_list']:\n            subject = spo['subject']\n            object = spo['object']\n            predicate = spo['predicate']\n            dct['triple_list'].append((subject, predicate, object))\n            # 计算 subject 实体位置\n            tokenized = self.tokenizer(subject, add_special_tokens=False)\n            sub_token = tokenized['input_ids']\n            sub_pos_id = self.get_pos_id(input_ids, sub_token)\n            if not sub_pos_id:\n                continue\n            sub_head_id, sub_tail_id = sub_pos_id\n            # 计算 object 实体位置\n            tokenized = self.tokenizer(object, add_special_tokens=False)\n            obj_token = tokenized['input_ids']\n            obj_pos_id = self.get_pos_id(input_ids, obj_token)\n            if not obj_pos_id:\n                continue\n            obj_head_id, obj_tail_id = obj_pos_id\n            # 数据组装\n            dct['sub_head_ids'].append(sub_head_id)\n            dct['sub_tail_ids'].append(sub_tail_id)\n            dct['triple_id_list'].append((\n                [sub_head_id, sub_tail_id],\n                self.rel2id[predicate],\n                [obj_head_id, obj_tail_id],\n            ))\n        return dct\n\n        # # 数据预处理部分已经格式化\n        # spo = info[\"spo_list\"]\n        # subject = spo['subject']\n        # object = spo['object']\n        # predicate = spo['predicate']\n        # dct['triple_list'].append((subject, predicate, object))\n        #\n        # tokenized = self.tokenizer(subject, add_special_tokens=False)\n        # sub_token = tokenized['input_ids']\n        # # print(input_ids)\n        # # print(sub_token)\n        # # exit()\n        # sub_pos_id = self.get_pos_id(input_ids, sub_token) #通过编码后的\n        #\n        # sub_head_id, sub_tail_id = sub_pos_id\n        # # 计算 object 实体位置\n        # tokenized = self.tokenizer(object, add_special_tokens=False)\n        # obj_token = tokenized['input_ids']\n        # obj_pos_id = self.get_pos_id(input_ids, obj_token)\n        #\n        # obj_head_id, obj_tail_id = obj_pos_id\n        # # 数据组装\n        # dct['sub_head_ids'].append(sub_head_id)\n        # dct['sub_tail_ids'].append(sub_tail_id)\n        #\n        # dct['triple_id_list'].append((\n        #     [sub_head_id, sub_tail_id],\n        #     self.rel2id[predicate],\n        #     [obj_head_id, obj_tail_id],\n        # ))\n        #\n        # return dct\n\nif __name__ == '__main__':\n    dataset = Dataset()\n    loader = data.DataLoader(dataset, shuffle=False, batch_size=2, collate_fn=dataset.collate_fn)\n    print(next(iter(loader)))\n    exit()","metadata":{"execution":{"iopub.status.busy":"2023-04-17T10:41:01.853618Z","iopub.execute_input":"2023-04-17T10:41:01.853926Z","iopub.status.idle":"2023-04-17T10:41:03.358615Z","shell.execute_reply.started":"2023-04-17T10:41:01.853898Z","shell.execute_reply":"2023-04-17T10:41:03.357307Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d03ed543aee94bed90f1f56149b74fc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7456e7ed6e6429ab3c19152d984594d"}},"metadata":{}},{"name":"stdout","text":"([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], ({'text': [\"Immobilization, while Paget's bone disease was present, and perhaps enhanced activation of dihydrotachysterol by rifampicin, could have led to increased calcium-release into the circulation.\", 'Intravenous azithromycin-induced ototoxicity.'], 'input_ids': [[101, 13280, 3702, 15197, 2734, 117, 1229, 3674, 1204, 112, 188, 6028, 3653, 1108, 1675, 117, 1105, 3229, 9927, 14915, 1104, 4267, 7889, 23632, 16339, 8992, 4648, 4063, 1118, 187, 8914, 19471, 27989, 1179, 117, 1180, 1138, 1521, 1106, 2569, 15355, 118, 1836, 1154, 1103, 9097, 119, 102], [101, 1107, 4487, 7912, 2285, 170, 5303, 1582, 16071, 1183, 16430, 118, 10645, 184, 2430, 2430, 8745, 9041, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'offset_mapping': [[(0, 0), (0, 2), (2, 4), (4, 7), (7, 14), (14, 15), (16, 21), (22, 26), (26, 27), (27, 28), (28, 29), (30, 34), (35, 42), (43, 46), (47, 54), (54, 55), (56, 59), (60, 67), (68, 76), (77, 87), (88, 90), (91, 93), (93, 95), (95, 97), (97, 100), (100, 103), (103, 107), (107, 109), (110, 112), (113, 114), (114, 116), (116, 119), (119, 122), (122, 123), (123, 124), (125, 130), (131, 135), (136, 139), (140, 142), (143, 152), (153, 160), (160, 161), (161, 168), (169, 173), (174, 177), (178, 189), (189, 190), (0, 0)], [(0, 0), (0, 2), (2, 5), (5, 8), (8, 11), (12, 13), (13, 15), (15, 17), (17, 20), (20, 21), (21, 24), (24, 25), (25, 32), (33, 34), (34, 36), (36, 38), (38, 40), (40, 44), (44, 45), (0, 0)]], 'triple_list': [[('dihydrotachysterol', 'causes', 'increased calcium-release')], [('azithromycin', 'causes', 'ototoxicity')]]}, {'head_seq': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'tail_seq': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}), ({'heads_seq': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'tails_seq': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}, {'heads_mx': [[[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]], [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]], 'tails_mx': [[[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]], [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]]}))\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import BertModel\nimport torch\nimport torch.nn.functional as F\n\n# 忽略 transformers 警告\nfrom transformers import logging\nlogging.set_verbosity_error()\n\n\nclass CasRel(nn.Module):\n\n    #初始化model\n    def __init__(self):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(BERT_MODEL_NAME)\n        # 冻结Bert参数，只训练下游模型\n        for name, param in self.bert.named_parameters():\n            param.requires_grad = False\n\n        #定义CasRel网络\n        self.sub_head_linear = nn.Linear(BERT_DIM, 1)   #sub只需要一维即\n        self.sub_tail_linear = nn.Linear(BERT_DIM, 1)\n\n        self.obj_head_linear = nn.Linear(BERT_DIM, REL_SIZE)  #预测的obj矩阵需要REL_SIZE维计\n        self.obj_tail_linear = nn.Linear(BERT_DIM, REL_SIZE)\n\n\n    #subject头尾标记预测\n    def get_encoded_text(self, input_ids, mask):\n        return self.bert(input_ids, attention_mask=mask)[0]\n\n    def get_subs(self, encoded_text):\n        #encoded_text(b,c,768) -> (b,c,1)\n        #对每个单词进行sigmoid预测\n        pred_sub_head = torch.sigmoid(self.sub_head_linear(encoded_text))\n        pred_sub_tail = torch.sigmoid(self.sub_tail_linear(encoded_text))\n\n        return pred_sub_head, pred_sub_tail\n\n    def get_objs_for_specific_sub(self, encoded_text, sub_head_seq, sub_tail_seq):  # 获得预测的obj-rel矩阵\n        # sub_head_seq.shape (b, c) -> (b, 1, c)\n        sub_head_seq = sub_head_seq.unsqueeze(1).float()\n        sub_tail_seq = sub_tail_seq.unsqueeze(1).float()\n\n        # encoded_text.shape (b, c, 768)\n        sub_head = torch.matmul(sub_head_seq, encoded_text)   #获得head和tail的编码并加在encoded_text中\n        sub_tail = torch.matmul(sub_tail_seq, encoded_text)\n\n        encoded_text = encoded_text + (sub_head + sub_tail) / 2\n\n        # encoded_text.shape (b, c, 768)\n        pred_obj_head = torch.sigmoid(self.obj_head_linear(encoded_text))\n        pred_obj_tail = torch.sigmoid(self.obj_tail_linear(encoded_text))\n\n        # shape (b, c, REL_SIZE)\n        return pred_obj_head, pred_obj_tail\n\n    def forward(self, input, mask):\n\n        input_ids, sub_head_seq, sub_tail_seq = input\n        encoded_text = self.get_encoded_text(input_ids, mask)\n        pred_sub_head, pred_sub_tail = self.get_subs(encoded_text)\n\n\n        input_ids, sub_head_seq, sub_tail_seq = input\n        encoded_text = self.get_encoded_text(input_ids, mask)\n\n        # 预测subject首尾序列\n        pred_sub_head, pred_sub_tail = self.get_subs(encoded_text)\n\n        # 预测relation-object矩阵\n        pred_obj_head, pred_obj_tail = self.get_objs_for_specific_sub(encoded_text, sub_head_seq, sub_tail_seq)\n\n        return encoded_text, (pred_sub_head, pred_sub_tail, pred_obj_head, pred_obj_tail)\n\n    def loss_fn(self, true_y, pred_y, mask):\n\n        def calc_loss(pred, true, mask):\n            true = true.float()\n\n            # pred.shape (b, c, 1) -> (b, c)\n            pred = pred.squeeze(-1)\n            weight = torch.where(true > 0, CLS_WEIGHT_COEF[1], CLS_WEIGHT_COEF[0])  # 分配权重\n\n\n            loss = F.binary_cross_entropy(pred, true, weight=weight, reduction='none')\n\n\n            if loss.shape != mask.shape:\n                mask = mask.unsqueeze(-1)\n\n            return torch.sum(loss * mask) / torch.sum(mask)  #通过与mask相乘将pad补充的元素损失进行归0\n\n        pred_sub_head, pred_sub_tail, pred_obj_head, pred_obj_tail = pred_y\n        true_sub_head, true_sub_tail, true_obj_head, true_obj_tail = true_y\n\n\n        return calc_loss(pred_sub_head, true_sub_head, mask) * SUB_WEIGHT_COEF + \\\n               calc_loss(pred_sub_tail, true_sub_tail, mask) * SUB_WEIGHT_COEF + \\\n               calc_loss(pred_obj_head, true_obj_head, mask) + \\\n               calc_loss(pred_obj_tail, true_obj_tail, mask)\n\n\n\n\n\n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-17T10:41:03.360790Z","iopub.execute_input":"2023-04-17T10:41:03.361147Z","iopub.status.idle":"2023-04-17T10:41:05.634224Z","shell.execute_reply.started":"2023-04-17T10:41:03.361105Z","shell.execute_reply":"2023-04-17T10:41:05.633021Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"\nfrom torch.utils import data\n\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n\ndef get_triple_list(sub_head_ids, sub_tail_ids, model, encoded_text, text, mask, offset_mapping):\n    id2rel, _ = get_rel()\n    triple_list = []\n    for sub_head_id in sub_head_ids:\n        sub_tail_ids = sub_tail_ids[sub_tail_ids >= sub_head_id]\n        if len(sub_tail_ids) == 0:\n            continue\n        sub_tail_id = sub_tail_ids[0]\n        if mask[sub_head_id] == 0 or mask[sub_tail_id] == 0:\n            continue\n        # 根据位置信息反推出 subject 文本内容\n        sub_head_pos_id = offset_mapping[sub_head_id][0]\n        sub_tail_pos_id = offset_mapping[sub_tail_id][1]\n        subject_text = text[sub_head_pos_id:sub_tail_pos_id]\n        # 根据 subject 计算出对应 object 和 relation\n        sub_head_seq = torch.tensor(multihot(len(mask), sub_head_id)).to(DEVICE)\n        sub_tail_seq = torch.tensor(multihot(len(mask), sub_tail_id)).to(DEVICE)\n\n        pred_obj_head, pred_obj_tail = model.get_objs_for_specific_sub(\\\n            encoded_text.unsqueeze(0), sub_head_seq.unsqueeze(0), sub_tail_seq.unsqueeze(0))\n        # 按分类找对应关系\n        pred_obj_head = pred_obj_head[0].T\n        pred_obj_tail = pred_obj_tail[0].T\n        for j in range(len(pred_obj_head)):\n            obj_head_ids = torch.where(pred_obj_head[j] > OBJ_HEAD_BAR)[0]\n            obj_tail_ids = torch.where(pred_obj_tail[j] > OBJ_TAIL_BAR)[0]\n            for obj_head_id in obj_head_ids:\n                obj_tail_ids = obj_tail_ids[obj_tail_ids >= obj_head_id]\n                if len(obj_tail_ids) == 0:\n                    continue\n                obj_tail_id = obj_tail_ids[0]\n                if mask[obj_head_id] == 0 or mask[obj_tail_id] == 0:\n                    continue\n                # 根据位置信息反推出 object 文本内容，mapping中已经有移位，不需要再加1\n                obj_head_pos_id = offset_mapping[obj_head_id][0]\n                obj_tail_pos_id = offset_mapping[obj_tail_id][1]\n                object_text = text[obj_head_pos_id:obj_tail_pos_id]\n                triple_list.append((subject_text, id2rel[j], object_text))\n    return list(set(triple_list))\n\n\n\ndef report(model, encoded_text, pred_y, batch_text, batch_mask):\n    # 计算三元结构，和统计指标\n    pred_sub_head, pred_sub_tail, _, _ = pred_y\n    true_triple_list = batch_text['triple_list']\n    pred_triple_list = []\n\n    correct_num, predict_num, gold_num = 0, 0, 0\n\n    # 遍历batch\n    for i in range(len(pred_sub_head)):\n        text = batch_text['text'][i]\n        true_triple_item = true_triple_list[i]\n        mask = batch_mask[i]\n        offset_mapping = batch_text['offset_mapping'][i]\n\n        sub_head_ids = torch.where(pred_sub_head[i] > SUB_HEAD_BAR)[0]\n        sub_tail_ids = torch.where(pred_sub_tail[i] > SUB_TAIL_BAR)[0]\n\n        pred_triple_item = get_triple_list(sub_head_ids, sub_tail_ids, model, \\\n            encoded_text[i], text, mask, offset_mapping)\n\n        # 统计个数\n        correct_num += len(set(true_triple_item) & set(pred_triple_item))\n        predict_num += len(set(pred_triple_item))\n        gold_num += len(set(true_triple_item))\n\n        pred_triple_list.append(pred_triple_item)\n\n    precision = correct_num / (predict_num + EPS)\n    recall = correct_num / (gold_num + EPS)\n    f1_score = 2 * precision * recall / (precision + recall + EPS)\n    print('\\tcorrect_num:', correct_num, 'predict_num:', predict_num, 'gold_num:', gold_num)\n    print('\\tprecision:%.3f' % precision, 'recall:%.3f' % recall, 'f1_score:%.3f' % f1_score)\n\nif __name__ == '__main__':\n    model = CasRel().to(DEVICE)\n    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n    dataset = Dataset()\n\n    for e in range(EPOCH):\n        loader = data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=dataset.collate_fn)\n        for b, (batch_mask, batch_x, batch_y) in enumerate(loader):\n\n            # print(batch_x)\n            # exit()\n\n            batch_text, batch_sub_rnd = batch_x\n            batch_sub, batch_obj_rel = batch_y\n\n            # 整理input数据并预测\n            input_mask = torch.tensor(batch_mask).to(DEVICE)\n\n            input = (\n                torch.tensor(batch_text['input_ids']).to(DEVICE),\n                torch.tensor(batch_sub_rnd['head_seq']).to(DEVICE),\n                torch.tensor(batch_sub_rnd['tail_seq']).to(DEVICE),\n            )\n            encoded_text, pred_y = model(input, input_mask)\n\n            # 整理target数据并计算损失\n            true_y = (\n                torch.tensor(batch_sub['heads_seq']).to(DEVICE),\n                torch.tensor(batch_sub['tails_seq']).to(DEVICE),\n                torch.tensor(batch_obj_rel['heads_mx']).to(DEVICE),\n                torch.tensor(batch_obj_rel['tails_mx']).to(DEVICE),\n            )\n\n\n            loss = model.loss_fn(true_y, pred_y, input_mask)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            if b % 50 == 0:\n                print('>> epoch:', e, 'batch:', b, 'loss:', loss.item())\n            # print('>> epoch:', e, 'batch:', b, 'loss:', loss.item())\n            if b % 500 == 0:\n                report(model, encoded_text, pred_y, batch_text, batch_mask)\n\n        if e % 10 == 0:\n            torch.save(model, MODEL_DIR + f'model_{e}.pth')","metadata":{"execution":{"iopub.status.busy":"2023-04-17T10:41:05.639225Z","iopub.execute_input":"2023-04-17T10:41:05.641738Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b9dd192d4e24ae1929100ab00f55ea9"}},"metadata":{}},{"name":"stdout","text":">> epoch: 0 batch: 0 loss: 2.484619617462158\n\tcorrect_num: 0 predict_num: 975 gold_num: 150\n\tprecision:0.000 recall:0.000 f1_score:0.000\n","output_type":"stream"}]}]}