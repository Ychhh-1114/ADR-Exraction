{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n# @Author  : xmh\n# @Time    : 2021/3/11 16:53\n# @File    : config_rel.py\n\n\"\"\"\nfile description:：\n\n\"\"\"\nimport torch\n\nif torch.cuda.is_available():\n    USE_CUDA = True\n    print(\"USE_CUDA....\")\nelse:\n    USE_CUDA = False\n\n\nclass ConfigRel:\n    def __init__(self,\n                 lr=0.001,\n                 epochs=100,\n                 vocab_size=22000,  # 22000,\n                 embedding_dim=100,\n                 hidden_dim_lstm=128,\n                 num_layers=3,\n                 batch_size=32,\n                 layer_size=128,\n                 token_type_dim=8\n                 ):\n        self.lr = lr\n        self.epochs = epochs\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.hidden_dim_lstm = hidden_dim_lstm\n        self.num_layers = num_layers\n        self.batch_size = batch_size\n        self.layer_size = layer_size\n        self.token_type_dim = token_type_dim\n        self.relations = [\"causes\",'丈夫', '上映时间', '专业代码', '主持人', '主演', '主角', '人口数量', '作曲', '作者', '作词', '修业年限', '出品公司', '出版社',\n                          '出生地',\n                          '出生日期', '创始人', '制片人', '占地面积', '号', '嘉宾', '国籍', '妻子', '字', '官方语言', '导演', '总部地点', '成立日期',\n                          '所在城市', '所属专辑',\n                          '改编自', '朝代', '歌手', '母亲', '毕业院校', '民族', '气候', '注册资本', '海拔', '父亲', '目', '祖籍', '简称', '编剧', '董事长',\n                          '身高',\n                          '连载网站', '邮政编码', '面积', '首都']\n        self.num_relations = len(self.relations)\n        self.token_types_origin = ['Date', 'Number', 'Text', '书籍', '人物', '企业', '作品', '出版社', '历史人物', '国家', '图书作品', '地点',\n                                   '城市', '学校', '学科专业',\n                                   '影视作品', '景点', '机构', '歌曲', '气候', '生物', '电视综艺', '目', '网站', '网络小说', '行政区', '语言', '音乐专辑',\"drug\",\"adverse\"]\n        self.token_types = self.get_token_types()\n        self.num_token_type = len(self.token_types)\n        self.vocab_file = '../data/vocab.txt'\n        self.max_seq_length = 256\n        self.num_sample = 1480\n        \n        self.dropout_embedding = 0.1  # 从0.2到0.1\n        self.dropout_lstm = 0.1\n        self.dropout_lstm_output = 0.9\n        self.dropout_head = 0.9  # 只更改这个参数 0.9到0.5\n        self.dropout_ner = 0.8\n        self.use_dropout = True\n        self.threshold_rel = 0.95  # 从0.7到0.95\n        self.teach_rate = 0.2\n        self.ner_checkpoint_path = '../models/rel_cls/'\n        self.pretrained = False\n        self.pad_token_id = 0\n        self.rel_num = 500\n\n        self.pos_dim = 32\n    \n    def get_token_types(self):\n        token_type_bio = []\n        for token_type in self.token_types_origin:\n            token_type_bio.append('B-' + token_type)\n            token_type_bio.append('I-' + token_type)\n        token_type_bio.append('O')\n        \n        return token_type_bio\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-23T15:51:53.741621Z","iopub.execute_input":"2023-04-23T15:51:53.742051Z","iopub.status.idle":"2023-04-23T15:51:53.759693Z","shell.execute_reply.started":"2023-04-23T15:51:53.742002Z","shell.execute_reply":"2023-04-23T15:51:53.758316Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"USE_CUDA....\n","output_type":"stream"}]},{"cell_type":"code","source":"import  pandas as pd\n\n\n# def process_data(file_path):\n#     data = pd.read_csv(file_path)\n#     # print(data.columns)\n#     processed_data = []\n#\n#     for index, line in data.iterrows():\n#         dct = {\n#             \"text\": [],\n#             \"spo_list\": {\n#                 \"subject\": [],\n#                 \"object\": [],\n#                 \"predicate\": [],\n#             },\n#         }\n#         dct[\"text\"] = line[\"text\"]\n#         dct[\"spo_list\"][\"object\"] = line[\"effect\"]\n#         dct[\"spo_list\"][\"subject\"] = line[\"drug\"]\n#         dct[\"spo_list\"][\"predicate\"] = \"causes\"\n#         processed_data.append(dct)\n#\n#\n#     return processed_data\n#\n#\n#\ndef merge_data(data):\n    name_list = []\n    data_list = []\n\n    for each in data:\n        if each[\"text\"] not in name_list:\n            name_list.append(each[\"text\"])\n            data_list.append(each)\n        else:\n            index = name_list.index(each[\"text\"])\n            data_list[index][\"spo_list\"].append(each[\"spo_list\"][0])\n\n    return data_list\n\n\ndef process_data(file_path):\n    data = pd.read_csv(file_path)\n    # print(data.columns)\n    processed_data = []\n\n    for index, line in data.iterrows():\n        dct = {\n            \"text\": [],\n            \"spo_list\": [],\n        }\n        spo = {\n            \"subject\": \"\",\n            \"predicate\": \"\",\n            \"object\": \"\",\n            \"subject_type\":\"drug\",\n            \"object_type\":\"adverse\"\n\n        }\n\n        dct[\"text\"] = line[\"text\"]\n        spo[\"object\"] = line[\"effect\"]\n        spo[\"subject\"] = line[\"drug\"]\n        spo[\"predicate\"] = \"causes\"\n        dct[\"spo_list\"].append(({\"subject\":spo[\"subject\"],\"predicate\":spo[\"predicate\"],\"object\":spo[\"object\"],\"subject_type\":spo[\"subject_type\"],\"object_type\":spo[\"object_type\"]}))\n        processed_data.append(dct)\n\n    return merge_data(processed_data)\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T15:52:01.397187Z","iopub.execute_input":"2023-04-23T15:52:01.398297Z","iopub.status.idle":"2023-04-23T15:52:01.409738Z","shell.execute_reply.started":"2023-04-23T15:52:01.398256Z","shell.execute_reply":"2023-04-23T15:52:01.408536Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n# @Author  : xmh\n# @Time    : 2021/3/11 15:20\n# @File    : process_rel.py\n\n\"\"\"\nfile description:：\n\n\"\"\"\nimport json\nimport torch\n\nimport copy\nfrom transformers import BertTokenizer\nimport codecs\nfrom collections import defaultdict\n\n\nclass DataPreparationRel:\n    def __init__(self, config):\n        self.config = config\n        # self.get_token2id()\n        self.rel_cnt = defaultdict(int)\n    \n    def get_data(self, file_path, is_test=False):\n        data = []\n\n        datas = process_data(file_path)\n\n        for data_item in datas:\n            spo_list = data_item['spo_list']\n            text = data_item['text']\n            text = text.lower()\n            for spo_item in spo_list:\n                subject = spo_item[\"subject\"]\n                subject = subject.lower()\n                object = spo_item[\"object\"]\n                object = object.lower()\n                # 增加位置信息\n                # index_s = text.index(subject)\n                # index_o = text.index(object)\n                # position_s, position_o = [], []\n                # for i, word in enumerate(text):\n                #     if word not in self.word2id:\n                #         continue\n                #     position_s.append(i-index_s+self.config.max_seq_length*2)\n                #     position_o.append(i-index_o+self.config.max_seq_length*2)\n                if not is_test:\n                    relation = spo_item['predicate']\n                    if self.rel_cnt[relation] > self.config.rel_num:\n                        continue\n                    self.rel_cnt[relation] += 1\n\n                else:\n                    relation = []\n                # sentence_cls = '$'.join([subject, object, text.replace(subject, '#'*len(subject)).replace(object, '#'*len(object))])\n                sentence_cls = ''.join([subject, object, text])\n                # sentence_cls = text\n                item = {'sentence_cls': sentence_cls, 'relation': relation, 'text': text,\n                        'subject': subject, 'object': object}  # 'position_s': position_s, 'position_o': position_o}\n                data.append(item)\n            # 添加负样本\n            sentence_neg = '$'.join([object, subject, text])\n            # sentence_neg = '$'.join(\n            #     [object, subject, text.replace(subject, '#' * len(subject)).replace(object, '#' * len(object))])\n            item_neg = {'sentence_cls': sentence_neg, 'relation': 'N', 'text': text,\n                        'subject': object, 'object': subject}\n            data.append(item_neg)\n\n        dataset = Dataset(data)\n        if is_test:\n            dataset.is_test = True\n        data_loader = torch.utils.data.DataLoader(\n            dataset=dataset,\n            batch_size=self.config.batch_size,\n            collate_fn=dataset.collate_fn,\n            shuffle=True,\n            drop_last=True\n        )\n\n        return data_loader\n\n\n    def get_token2id(self):\n        self.word2id = {}\n        with codecs.open('../data/vec.txt', 'r', encoding='utf-8') as f:\n            cnt = 0\n            for line in f.readlines():\n                self.word2id[line.split()[0]] = cnt\n                cnt += 1\n\n    def get_train_dev_data(self, path_train=None, path_dev=None, path_test=None, is_test=False):\n        train_loader, dev_loader, test_loader = None, None, None\n        if path_train is not None:\n            train_loader = self.get_data(path_train)\n        if path_dev is not None:\n            dev_loader = self.get_data(path_dev)\n        if path_test is not None:\n            test_loader = self.get_data(path_test, is_test=True)\n    \n        return train_loader, dev_loader, test_loader\n    \n    \nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, data):\n        self.data = copy.deepcopy(data)\n        self.is_test = False\n        with open('/kaggle/input/lstm-crf-adr/rel2id.json', 'r', encoding='utf-8') as f:\n            self.rel2id = json.load(f)\n        vocab_file = 'dmis-lab/biobert-base-cased-v1.2'\n        self.bert_tokenizer = BertTokenizer.from_pretrained(vocab_file)\n\n        # vocab_file = '../data/vec.txt'\n        # self.get_token2id()\n\n    # def get_token2id(self):\n    #     self.word2id = {}\n    #     with codecs.open('../data/vec.txt', 'r', encoding='utf-8') as f:\n    #         cnt = 0\n    #         for line in f.readlines():\n    #             self.word2id[line.split()[0]] = cnt\n    #             cnt += 1\n\n    def __getitem__(self, index):\n        sentence_cls = self.data[index]['sentence_cls']\n        relation = self.data[index]['relation']\n        text = self.data[index]['text']\n        subject = self.data[index]['subject']\n        object = self.data[index]['object']\n        # position_s = self.data[index]['position_s']\n        # position_o = self.data[index]['position_o']\n        \n        data_info = {}\n        for key in self.data[0].keys():\n            if key in locals():\n                data_info[key] = locals()[key]\n        \n        return data_info\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def collate_fn(self, data_batch):\n        def merge(sequences):\n            lengths = [len(seq) for seq in sequences]\n            max_length = max(lengths)\n            # padded_seqs = torch.zeros(len(sequences), max_length)\n            padded_seqs = torch.zeros(len(sequences), max_length)\n            tmp_pad = torch.ones(1, max_length)\n            mask_tokens = torch.zeros(len(sequences), max_length)\n            for i, seq in enumerate(sequences):\n                end = lengths[i]\n                seq = torch.LongTensor(seq)\n                if len(seq) != 0:\n                    padded_seqs[i, :end] = seq[:end]\n                    mask_tokens[i, :end] = tmp_pad[0, :end]\n            \n            return padded_seqs, mask_tokens\n        \n        item_info = {}\n        for key in data_batch[0].keys():\n            item_info[key] = [d[key] for d in data_batch]\n\n        # 转化为数值\n        sentence_cls = [self.bert_tokenizer.encode(sentence, add_special_tokens=True) for sentence in item_info['sentence_cls']]\n        # sentence_cls = [[] for _ in range(len(item_info['sentence_cls']))]\n        # for i, sentence in enumerate(item_info['sentence_cls']):\n        #     tmp = []\n        #     for c in sentence:\n        #         if c in self.word2id:\n        #             tmp.append(self.word2id[c])\n        #     sentence_cls[i] = tmp\n\n        if not self.is_test:\n            relation = torch.Tensor([self.rel2id[rel] for rel in item_info['relation']]).to(torch.int64)\n        \n        # 批量数据对齐\n        sentence_cls, mask_tokens = merge(sentence_cls)\n        sentence_cls = sentence_cls.to(torch.int64)\n        mask_tokens = mask_tokens.to(torch.int64)\n        relation = relation.to(torch.int64)\n        # position_s, _ = merge(item_info['position_s'])\n        # position_o, _ = merge(item_info['position_o'])\n        if USE_CUDA:\n            sentence_cls = sentence_cls.contiguous().cuda()\n            mask_tokens = mask_tokens.contiguous().cuda()\n            # position_s = position_s.contiguous().cuda()\n            # position_o = position_o.contiguous().cuda()\n        else:\n            sentence_cls = sentence_cls.contiguous()\n            mask_tokens = mask_tokens.contiguous()\n            # position_s = position_s.contiguous()\n            # position_o = position_o.contiguous()\n        if not self.is_test:\n            if USE_CUDA:\n                relation = relation.contiguous().cuda()\n            else:\n                relation = relation.contiguous()\n\n        data_info = {\"mask_tokens\": mask_tokens.to(torch.uint8)}\n        data_info['text'] = item_info['text']\n        data_info['subject'] = item_info['subject']\n        data_info['object'] = item_info['object']\n        for key in item_info.keys():\n            if key in locals():\n                data_info[key] = locals()[key]\n        \n        return data_info\n        \n        \nif __name__ == '__main__':\n    config = ConfigRel()\n    process = DataPreparationRel(config)\n    #train_loader, dev_loader, test_loader = process.get_train_dev_data('adr-train.csv')\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T15:52:06.906064Z","iopub.execute_input":"2023-04-23T15:52:06.906645Z","iopub.status.idle":"2023-04-23T15:52:06.936899Z","shell.execute_reply.started":"2023-04-23T15:52:06.906607Z","shell.execute_reply":"2023-04-23T15:52:06.935777Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n# @Author  : xmh\n# @Time    : 2021/3/11 16:36\n# @File    : model_rel.py\n\n\"\"\"\nfile description:：\n\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef setup_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)   # 使用相同的初始化种子，保证每次初始化结果一直，便于调试\n\n\nclass Attention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        setup_seed(1)\n        # self.query = nn.Parameter(torch.randn(1, config.hidden_dim_lstm))  # [batch, 1, hidden_dim]\n    \n    # def forward(self, H):\n    #     M = torch.tanh(H)  # H [batch_size, sentence_length, hidden_dim_lstm]\n    #     attention_prob = torch.matmul(M, self.query.transpose(-1, -2))  # [batch_size, sentence_length, 1]\n    #     alpha = F.softmax(attention_prob,dim=-1)\n    #     attention_output = torch.matmul(alpha.transpose(-1, -2), H)  # [batch_size, 1, hidden_dim_lstm]\n    #     attention_output = attention_output.squeeze(axis=1)\n    #     attention_output = torch.tanh(attention_output)\n    #     return attention_output\n    \n    def forward(self, output_lstm, hidden_lstm):\n        hidden_lstm = torch.sum(hidden_lstm, dim=0)\n        att_weights = torch.matmul(output_lstm, hidden_lstm.unsqueeze(2)).squeeze(2)\n        alpha = F.softmax(att_weights, dim=1)\n        new_hidden = torch.matmul(output_lstm.transpose(-1, -2), alpha.unsqueeze(2)).squeeze(2)\n        \n        return new_hidden\n\nclass AttBiLSTM(nn.Module):\n    def __init__(self, config, embedding_pre=None):\n        super().__init__()\n        setup_seed(1)\n        self.embedding_dim = config.embedding_dim\n        self.vocab_size = config.vocab_size\n        self.hidden_dim = config.hidden_dim_lstm\n        self.num_layers = config.num_layers\n        self.batch_size = config.batch_size\n        self.embed_dropout = nn.Dropout(config.dropout_embedding)\n        self.lstm_dropout = nn.Dropout(config.dropout_lstm_output)\n        self.pretrained = config.pretrained\n        self.config = config\n        self.relation_embed_layer = nn.Embedding(config.num_relations, self.hidden_dim)\n        self.relations = torch.Tensor([i for i in range(config.num_relations)])\n        if USE_CUDA:\n            self.relations = self.relations.cuda()\n        self.relation_bias = nn.Parameter(torch.randn(config.num_relations))\n        \n        assert (self.pretrained is True and embedding_pre is not None) or \\\n               (self.pretrained is False and embedding_pre is None), \"预训练必须有训练好的embedding_pre\"\n        # 定义网络层\n        # 对于关系抽取，命名实体识别和关系抽取共享编码层\n        if self.pretrained:\n            # self.word_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_pre), freeze=False)\n            self.word_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_pre), freeze=False)\n        else:\n            self.word_embedding = nn.Embedding(config.vocab_size, config.embedding_dim, padding_idx=config.pad_token_id)\n        \n        # self.pos1_embedding = nn.Embedding(config.pos_size, config.embedding_dim)\n        # self.pos2_embedding = nn.Embedding(config.pos_size, config.embedding_dim)\n        self.gru = nn.GRU(config.embedding_dim+2*config.pos_dim, config.hidden_dim_lstm, num_layers=config.num_layers, batch_first=True, bidirectional=True,\n                          dropout=config.dropout_lstm)\n        self.attention_layer = Attention(config)\n        # self.classifier = nn.Linear(config.hidden_dim_lstm, config.num_relations)\n\n        if USE_CUDA:\n            self.weights_rel = (torch.ones(self.config.num_relations) * 6).cuda()\n        else:\n            self.weights_rel = torch.ones(self.config.num_relations) * 6\n        # self.weights_rel[9], self.weights_rel[13], self.weights_rel[14], self.weights_rel[46] = 100, 100, 100, 100\n        self.weights_rel[0] = 1\n        self.hidden_init = torch.randn(2 * self.num_layers, self.batch_size, self.hidden_dim)\n        if USE_CUDA:\n            self.hidden_init = self.hidden_init.cuda()\n        # self.pos_embedding_layer = nn.Embedding(config.max_seq_length*4, config.pos_dim)\n    \n    def forward(self, data_item, is_test=False):\n\n        word_embeddings = self.word_embedding(data_item['sentence_cls'].to(torch.int64))\n        # pos1_embeddings = self.pos_embedding_layer(data_item['position_s'].to(torch.int64))\n        # pos2_embeddings = self.pos_embedding_layer(data_item['position_o'].to(torch.int64))\n        # embeddings = torch.cat((word_embeddings, pos1_embeddings, pos2_embeddings), 2)  # batch_size, seq, word_dim+2*pos_dim\n        embeddings = word_embeddings\n        if self.config.use_dropout:\n            embeddings = self.embed_dropout(embeddings)\n\n        output, h_n = self.gru(embeddings, self.hidden_init)\n        if self.config.use_dropout:\n            output = self.lstm_dropout(output)\n        attention_input = output[:, :, :self.hidden_dim] + output[:, :, self.hidden_dim:]\n        attention_output = self.attention_layer(attention_input, h_n)\n        # hidden_cls = torch.tanh(attention_output)\n        # output_cls = self.classifier(attention_output)\n        relation_embeds = self.relation_embed_layer(self.relations.to(torch.int64))\n        # res = torch.add(torch.matmul(attention_output, relation_embeds.transpose(-1, -2)), self.relation_bias)\n        res = torch.matmul(attention_output, relation_embeds.transpose(-1, -2))\n\n        if not is_test:\n            loss = F.cross_entropy(res, data_item['relation'], self.weights_rel)  # loss = F.cross_entropy(attention_output, data_item['relation'])\n            # loss /= self.config.batch_size\n        res = F.softmax(res, -1)\n        pred = res.argmax(dim=-1)\n        if is_test:\n            return pred\n        return loss, pred\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n# @Author  : xmh\n# @Time    : 2021/3/11 16:38\n# @File    : trainer_rel.py\n\n\"\"\"\nfile description:：\n\n\"\"\"\n!pip install neptune\n# coding=utf-8\nimport sys\n\n\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\n\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\n\nimport numpy as np\nimport codecs\nfrom transformers import BertForSequenceClassification\nimport neptune\n\n\nclass Trainer:\n    def __init__(self,\n                 model,\n                 config,\n                 train_dataset=None,\n                 dev_dataset=None,\n                 test_dataset=None\n                 ):\n        self.model = model\n        self.config = config\n        self.train_dataset = train_dataset\n        self.dev_dataset = dev_dataset\n        self.test_dataset = test_dataset\n        \n        if USE_CUDA:\n            self.model = self.model.cuda()\n        \n        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=config.lr)\n        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5,\n                                                                    patience=8, min_lr=1e-5, verbose=True)\n        self.get_id2rel()\n        \n    def train(self):\n        print('STARTING TRAIN...')\n        self.num_sample_total = len(self.train_dataset) * self.config.batch_size\n        loss_eval_best = 1e8\n        for epoch in range(self.config.epochs):\n            print(\"Epoch: {}\".format(epoch))\n            pbar = tqdm(enumerate(self.train_dataset), total=len(self.train_dataset))\n            loss_rel_total = 0.0\n            self.optimizer.zero_grad()\n            # with torch.no_grad():\n            for i, data_item in pbar:\n                loss_rel, pred_rel = self.model(data_item)\n                loss_rel.backward()\n                self.optimizer.step()\n\n                loss_rel_total += loss_rel\n            loss_rel_train_ave = loss_rel_total / self.num_sample_total\n            print(\"train rel loss: {0}\".format(loss_rel_train_ave))\n            neptune.log_metric(\"train rel loss\", loss_rel_train_ave)\n            if (epoch + 1) % 1 == 0:\n                loss_rel_ave = self.evaluate()\n\n            if epoch > 0 and (epoch+1) % 2 == 0:\n                if loss_rel_ave < loss_eval_best:\n                    loss_eval_best = loss_rel_ave\n                    torch.save({\n                        'epoch': epoch + 1, 'state_dict': self.model.state_dict(), 'loss_rel_best': loss_eval_best,\n                        'optimizer': self.optimizer.state_dict(),\n                    },\n                        self.config.ner_checkpoint_path + str(epoch) + 'm-' + 'loss' +\n                        str(\"%.2f\" % loss_rel_ave) + 'ccks2019_rel.pth'\n                    )\n    \n    def evaluate(self):\n        print('STARTING EVALUATION...')\n        self.model.train(False)\n        pbar_dev = tqdm(enumerate(self.dev_dataset), total=len(self.dev_dataset))\n    \n        loss_rel_total = 0\n        for i, data_item in pbar_dev:\n            loss_rel, pred_rel = self.model(data_item)\n            loss_rel_total += loss_rel\n        \n        self.model.train(True)\n        loss_rel_ave = loss_rel_total / (len(self.dev_dataset) * self.config.batch_size)\n        print(\"eval rel loss: {0}\".format(loss_rel_ave))\n        \n        print(data_item['text'][1])\n        print(\"subject: {0}, object：{1}\".format(data_item['subject'][1], data_item['object'][1]))\n        print(\"object rel: {}\".format(self.id2rel[int(data_item['relation'][1])]))\n        print(\"predict rel: {}\".format(self.id2rel[int(pred_rel[1])]))\n        return loss_rel_ave\n    \n    def get_id2rel(self):\n        self.id2rel = {}\n        for i, rel in enumerate(self.config.relations):\n            self.id2rel[i] = rel\n\n    def predict(self):\n        print('STARTING PREDICTING...')\n        self.model.train(False)\n        pbar = tqdm(enumerate(self.test_dataset), total=len(self.test_dataset))\n        for i, data_item in pbar:\n            pred_rel = self.model(data_item, is_test=True)\n        self.model.train(True)\n        rel_pred = [[] for _ in range(len(pred_rel))]\n        for i in range(len(pred_rel)):\n            # for item in pred_rel[i]:\n            rel_pred[i].append(self.id2rel[int(pred_rel[i])])\n        return rel_pred\n\n    def bert_train(self):\n        print('STARTING TRAIN...')\n        self.num_sample_total = len(self.train_dataset) * self.config.batch_size\n        acc_best = 0.0\n        for epoch in range(self.config.epochs):\n            print(\"Epoch: {}\".format(epoch))\n            pbar = tqdm(enumerate(self.train_dataset), total=len(self.train_dataset))\n            loss_rel_total = 0.0\n            # self.optimizer.zero_grad()\n            correct = 0\n            # with torch.no_grad():\n            for i, data_item in pbar:\n                self.optimizer.zero_grad()\n                output = self.model(data_item['sentence_cls'], attention_mask=data_item['mask_tokens'], labels=data_item['relation'])\n                loss_rel, logits = output[0], output[1]\n                loss_rel.backward()\n                self.optimizer.step()\n\n                _, pred_rel = torch.max(logits.data, 1)\n                correct += pred_rel.data.eq(data_item['relation'].data).cpu().sum().numpy()\n\n                loss_rel_total += loss_rel\n            loss_rel_train_ave = loss_rel_total / self.num_sample_total\n            print(\"train rel loss: {0}\".format(loss_rel_train_ave))\n            # neptune.log_metric(\"train rel loss\", loss_rel_train_ave)\n            print(\"precision_score: {0}\".format(correct / self.num_sample_total))\n            if (epoch + 1) % 1 == 0:\n                acc_eval = self.bert_evaluate()\n\n            if epoch > 0 and (epoch + 1) % 2 == 0:\n                if acc_eval > acc_best:\n                    acc_best = acc_eval\n                    torch.save({\n                        'epoch': epoch + 1, 'state_dict': self.model.state_dict(), 'acc_best': acc_best,\n                        'optimizer': self.optimizer.state_dict(),\n                    },\n                        str(epoch) + 'm-' + 'acc' +\n                        str(\"%.2f\" % acc_best) + 'ccks2019_rel.pth'\n                    )\n\n    def bert_evaluate(self):\n        print('STARTING EVALUATION...')\n        self.model.train(False)\n        pbar_dev = tqdm(enumerate(self.dev_dataset), total=len(self.dev_dataset))\n\n        loss_rel_total = 0\n        correct = 0\n        with torch.no_grad():\n            for i, data_item in pbar_dev:\n                output = self.model(data_item['sentence_cls'], attention_mask=data_item['mask_tokens'], labels=data_item['relation'])\n                loss_rel, logits = output[0], output[1]\n                _, pred_rel = torch.max(logits.data, 1)\n                loss_rel_total += loss_rel\n                correct += pred_rel.data.eq(data_item['relation'].data).cpu().sum().numpy()\n\n        self.model.train(True)\n        loss_rel_ave = loss_rel_total / (len(self.dev_dataset) * self.config.batch_size)\n        correct_ave = correct / (len(self.dev_dataset) * self.config.batch_size)\n        print(\"eval rel loss: {0}\".format(loss_rel_ave))\n        print(\"precision_score: {0}\".format(correct_ave))\n\n        print(data_item['text'][1])\n        print(\"subject: {0}, object：{1}\".format(data_item['subject'][1], data_item['object'][1]))\n        print(\"object rel: {}\".format(self.id2rel[int(data_item['relation'][1])]))\n        print(\"predict rel: {}\".format(self.id2rel[int(pred_rel[1])]))\n        return correct_ave\n\n    def bert_predict(self):\n        print('STARTING PREDICTING...')\n        self.model.train(False)\n        pbar = tqdm(enumerate(self.test_dataset), total=len(self.test_dataset))\n        for i, data_item in pbar:\n            output = self.model(data_item['sentence_cls'], attention_mask=data_item['mask_tokens'])\n            logits = output[0]\n            _, pred_rel = torch.max(logits.data, 1)\n        self.model.train(True)\n        # rel_pred = [[] for _ in range(len(pred_rel))]\n        rel_pred = []\n        for i in range(len(pred_rel)):\n            # for item in pred_rel[i]:\n            rel_pred.append(self.id2rel[int(pred_rel[i])])\n        return rel_pred\n\n\n\n\n\nif __name__ == '__main__':\n\n    print(\"Run EntityRelationExtraction REL BERT ...\")\n    config = ConfigRel()\n    model = BertForSequenceClassification.from_pretrained('dmis-lab/biobert-base-cased-v1.2', num_labels=config.num_relations)\n    data_processor = DataPreparationRel(config)\n    train_loader, dev_loader, test_loader = data_processor.get_train_dev_data(\n        '/kaggle/input/lstm-crf-adr/adr-train.csv',\n    '/kaggle/input/lstm-crf-adr/adr-test.csv',\n    '/kaggle/input/lstm-crf-adr/adr-test.csv',)\n    # train_loader, dev_loader, test_loader = data_processor.get_train_dev_data('../data/train_data_small.json')\n    trainer = Trainer(model, config, train_loader, dev_loader, test_loader)\n    trainer.bert_train()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}