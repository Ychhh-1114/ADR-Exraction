{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n# @Author  : xmh\n# @Time    : 2021/3/3 20:42\n# @File    : config_ner.py\n\n\"\"\"\nfile description:：\n\n\"\"\"\nimport torch\n\nif torch.cuda.is_available():\n    USE_CUDA = True\n    print(\"USE_CUDA....\")\nelse:\n    USE_CUDA = False\n\n\nclass ConfigNer:\n    def __init__(self,\n                 lr=0.001,\n                 epochs=100,\n                 vocab_size=220000,\n                 embedding_dim=100,\n                 hidden_dim_lstm=128,\n                 num_layers=3,\n                 batch_size=32,\n                 layer_size=128,\n                 token_type_dim=8\n                 ):\n        self.lr = lr\n        self.epochs = epochs\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.hidden_dim_lstm = hidden_dim_lstm\n        self.num_layers = num_layers\n        self.batch_size = batch_size\n        self.layer_size = layer_size\n        self.token_type_dim = token_type_dim\n        self.relations = [\"N\", '丈夫', '上映时间', '专业代码', '主持人', '主演', '主角', '人口数量', '作曲', '作者', '作词', '修业年限', '出品公司', '出版社', '出生地',\n                '出生日期', '创始人', '制片人', '占地面积', '号', '嘉宾', '国籍', '妻子', '字', '官方语言', '导演', '总部地点', '成立日期', '所在城市', '所属专辑',\n                '改编自', '朝代', '歌手', '母亲', '毕业院校', '民族', '气候', '注册资本', '海拔', '父亲', '目', '祖籍', '简称', '编剧', '董事长', '身高',\n                '连载网站', '邮政编码', '面积', '首都',\"causes\"]\n        self.num_relations = len(self.relations)\n        self.token_types_origin = ['Date', 'Number', 'Text', '书籍', '人物', '企业', '作品', '出版社', '历史人物', '国家', '图书作品', '地点', '城市', '学校', '学科专业',\n         '影视作品', '景点', '机构', '歌曲', '气候', '生物', '电视综艺', '目', '网站', '网络小说', '行政区', '语言', '音乐专辑',\"drug\",\"adverse\"]\n        self.token_types = self.get_token_types()\n        self.num_token_type = len(self.token_types)\n        self.vocab_file = '/kaggle/input/casrel-adr-data/vocab.txt'\n        self.max_seq_length = 256\n        self.num_sample = 204800\n\n        self.dropout_embedding = 0.1  # 从0.2到0.1\n        self.dropout_lstm = 0.1\n        self.dropout_lstm_output = 0.9\n        self.dropout_head = 0.9  # 只更改这个参数 0.9到0.5\n        self.dropout_ner = 0.8\n        self.use_dropout = True\n        self.threshold_rel = 0.65  # 从0.7到0.95\n        self.teach_rate = 0.2\n        self.ner_checkpoint_path = ''\n    \n    def get_token_types(self):\n        token_type_bio = []\n        for token_type in self.token_types_origin:\n            token_type_bio.append('B-' + token_type)\n            token_type_bio.append('I-' + token_type)\n        token_type_bio.append('O')\n        \n        return token_type_bio\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-23T10:12:51.562989Z","iopub.execute_input":"2023-04-23T10:12:51.563429Z","iopub.status.idle":"2023-04-23T10:12:51.578324Z","shell.execute_reply.started":"2023-04-23T10:12:51.563393Z","shell.execute_reply":"2023-04-23T10:12:51.576860Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"USE_CUDA....\n","output_type":"stream"}]},{"cell_type":"code","source":"import  pandas as pd\n\n\n# def process_data(file_path):\n#     data = pd.read_csv(file_path)\n#     # print(data.columns)\n#     processed_data = []\n#\n#     for index, line in data.iterrows():\n#         dct = {\n#             \"text\": [],\n#             \"spo_list\": {\n#                 \"subject\": [],\n#                 \"object\": [],\n#                 \"predicate\": [],\n#             },\n#         }\n#         dct[\"text\"] = line[\"text\"]\n#         dct[\"spo_list\"][\"object\"] = line[\"effect\"]\n#         dct[\"spo_list\"][\"subject\"] = line[\"drug\"]\n#         dct[\"spo_list\"][\"predicate\"] = \"causes\"\n#         processed_data.append(dct)\n#\n#\n#     return processed_data\n#\n#\n#\ndef merge_data(data):\n    name_list = []\n    data_list = []\n\n    for each in data:\n        if each[\"text\"] not in name_list:\n            name_list.append(each[\"text\"])\n            data_list.append(each)\n        else:\n            index = name_list.index(each[\"text\"])\n            data_list[index][\"spo_list\"].append(each[\"spo_list\"][0])\n\n    return data_list\n\n\ndef process_data(file_path):\n    data = pd.read_csv(file_path)\n    # print(data.columns)\n    processed_data = []\n\n    for index, line in data.iterrows():\n        dct = {\n            \"text\": [],\n            \"spo_list\": [],\n        }\n        spo = {\n            \"subject\": \"\",\n            \"predicate\": \"\",\n            \"object\": \"\",\n            \"subject_type\":\"drug\",\n            \"object_type\":\"adverse\"\n\n        }\n\n        dct[\"text\"] = line[\"text\"]\n        spo[\"object\"] = line[\"effect\"]\n        spo[\"subject\"] = line[\"drug\"]\n        spo[\"predicate\"] = \"causes\"\n        dct[\"spo_list\"].append(({\"subject\":spo[\"subject\"],\"predicate\":spo[\"predicate\"],\"object\":spo[\"object\"],\"subject_type\":spo[\"subject_type\"],\"object_type\":spo[\"object_type\"]}))\n        processed_data.append(dct)\n\n    return merge_data(processed_data)\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T10:12:51.615880Z","iopub.execute_input":"2023-04-23T10:12:51.616204Z","iopub.status.idle":"2023-04-23T10:12:51.627340Z","shell.execute_reply.started":"2023-04-23T10:12:51.616175Z","shell.execute_reply":"2023-04-23T10:12:51.626045Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n# @Author  : xmh\n# @Time    : 2021/3/3 14:31\n# @File    : process_ner.py\n\n\"\"\"\nfile description:：\n\n\"\"\"\n\n'''\n针对spo_list的主客体在原文的token进行标注，第一个字标注B-type,后面的字标注I-type，文本中其他词标注为O\n（先将所有文本标注为O，然后根据spo_list的内容，将对应位置覆盖）\n\n'''\nimport json\nimport torch\nimport copy\n\nimport numpy as np\n\n\n\nclass ModelDataPreparation:\n    def __init__(self, config):\n        self.config = config\n        self.get_type2id()\n    \n    def subject_object_labeling(self, spo_list, text, text_tokened):\n        # 在列表 k 中确定列表 q 的位置\n        def _index_q_list_in_k_list(q_list, k_list):\n            \"\"\"Known q_list in k_list, find index(first time) of q_list in k_list\"\"\"\n            q_list_length = len(q_list)\n            k_list_length = len(k_list)\n            for idx in range(k_list_length - q_list_length + 1):\n                t = [q == k for q, k in zip(q_list, k_list[idx: idx + q_list_length])]\n                # print(idx, t)\n                if all(t):\n                    # print(idx)\n                    idx_start = idx\n                    return idx_start\n\n        # 给主体和客体表上BIO分割式类型标签\n        def _labeling_type(subject_object, so_type):\n            so_tokened = [c for c in subject_object]\n            so_tokened_length = len(so_tokened)\n            idx_start = _index_q_list_in_k_list(q_list=so_tokened, k_list=text_tokened)\n            if idx_start is None:\n                tokener_error_flag = True\n                '''\n                实体: \"1981年\"  原句: \"●1981年2月27日，中国人口学会成立\"\n                so_tokened ['1981', '年']  text_tokened ['●', '##19', '##81', '年', '2', '月', '27', '日', '，', '中', '国', '人', '口', '学', '会', '成', '立']\n                so_tokened 无法在 text_tokened 找到！原因是bert_tokenizer.tokenize 分词增添 “##” 所致！\n                '''\n            else:  # 给实体开始处标 B 其它位置标 I\n                labeling_list[idx_start] = \"B-\" + so_type\n                if so_tokened_length == 2:\n                    labeling_list[idx_start + 1] = \"I-\" + so_type\n                elif so_tokened_length >= 3:\n                    labeling_list[idx_start + 1: idx_start + so_tokened_length] = [\"I-\" + so_type] * (\n                                so_tokened_length - 1)\n            return idx_start\n\n        labeling_list = [\"O\" for _ in range(len(text_tokened))]\n        have_error = False\n        for spo_item in spo_list:\n            subject = spo_item[\"subject\"]\n            subject_type = spo_item[\"subject_type\"]\n            object = spo_item[\"object\"]\n            subject, object = map(self.get_rid_unkonwn_word, (subject, object))\n            subject = list(map(lambda x: x.lower(), subject))\n            object = list(map(lambda x: x.lower(), object))\n            object_type = spo_item[\"object_type\"]\n            subject_idx_start = _labeling_type(subject, subject_type)\n            object_idx_start = _labeling_type(object, object_type)\n            if subject_idx_start is None or object_idx_start is None:\n                have_error = True\n                return labeling_list, have_error\n            #sample_cls = '$'.join([subject, object, text.replace(subject, '#'*len(subject)).replace(object, '#')])\n            #cls_list.append(sample_cls)\n        return labeling_list, have_error\n\n    def get_rid_unkonwn_word(self, text):\n        text_rid = []\n        for token in text:  # 删除不在vocab里面的词汇\n            if token in self.token2id.keys():\n                text_rid.append(token)\n        return text_rid\n    \n    def get_type2id(self):\n        self.token_type2id = {}\n        for i, token_type in enumerate(self.config.token_types):\n            self.token_type2id[token_type] = i\n        # with open('token_type2id.json', 'w', encoding='utf-8') as f:\n        #     json.dump(self.token_type2id, f, ensure_ascii=False)\n        # with open('rel2id.json', 'w', encoding='utf-8') as f:\n        #     json.dump(self.rel2id, f, ensure_ascii=False)\n        self.token2id = {}\n        with open(self.config.vocab_file, 'r', encoding='utf-8') as f:\n            cnt = 0\n            for line in f:\n                line = line.rstrip().split()\n                self.token2id[line[0]] = cnt\n                cnt += 1\n        self.token2id[' '] = cnt\n    \n    def get_data(self, file_path, is_test=False):\n        data = []\n        cnt = 0\n        datas = process_data(file_path)\n        for data_item in datas:\n            cnt += 1\n            if cnt > self.config.num_sample:\n                break\n            if not is_test:\n                spo_list = data_item['spo_list']\n            else:\n                spo_list = []\n            text = data_item['text']\n            text_tokened = [c.lower() for c in text]  # 中文使用简单的分词\n            token_type_list, token_type_origin = None, None\n\n            text_tokened = self.get_rid_unkonwn_word(text_tokened)\n            if not is_test:\n                token_type_list, have_error = self.subject_object_labeling(\n                    spo_list=spo_list, text=text, text_tokened=text_tokened\n                )\n                token_type_origin = token_type_list  # 保存没有数值化前的token_type\n                if have_error:\n                    continue\n            item = {'text_tokened': text_tokened, 'token_type_list': token_type_list}\n            item['text_tokened'] = [self.token2id[x] for x in item['text_tokened']]\n            if not is_test:\n                item['token_type_list'] = [self.token_type2id[x] for x in item['token_type_list']]\n            item['text'] = ''.join(text_tokened)  # 保存消除异常词汇的文本\n            item['spo_list'] = spo_list\n            item['token_type_origin'] = token_type_origin\n            data.append(item)\n        dataset = Dataset(data)\n        if is_test:\n            dataset.is_test = True\n        data_loader = torch.utils.data.DataLoader(\n            dataset=dataset,\n            batch_size=self.config.batch_size,\n            collate_fn=dataset.collate_fn,\n            drop_last=True\n        )\n        return data_loader\n\n\n    def get_train_dev_data(self, path_train=None, path_dev=None, path_test=None):\n        train_loader, dev_loader, test_loader = None, None, None\n        if path_train is not None:\n            train_loader = self.get_data(path_train)\n        if path_dev is not None:\n            dev_loader = self.get_data(path_dev)\n        if path_test is not None:\n            test_loader = self.get_data(path_test, is_test=True)\n        \n        return train_loader, dev_loader, test_loader\n        \n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, data):\n        self.data = copy.deepcopy(data)\n        self.is_test = False\n    \n    def __getitem__(self, index):\n        text_tokened = self.data[index]['text_tokened']\n        token_type_list = self.data[index]['token_type_list']\n        \n        data_info = {}\n        for key in self.data[0].keys():\n            # try:\n            #     data_info[key] = locals()[key]\n            # except KeyError:\n            #     print('{} cannot be found in locals()'.format(key))\n            if key in locals():\n                data_info[key] = locals()[key]\n\n        data_info['text'] = self.data[index]['text']\n        data_info['spo_list'] = self.data[index]['spo_list']\n        data_info['token_type_origin'] = self.data[index]['token_type_origin']\n        return data_info\n    \n    def __len__(self):\n        return len(self.data)\n      \n    def collate_fn(self, data_batch):\n        \n        def merge(sequences):\n            lengths = [len(seq) for seq in sequences]\n            max_length = max(lengths)\n            # padded_seqs = torch.zeros(len(sequences), max_length)\n            padded_seqs = torch.zeros(len(sequences), max_length)\n            tmp_pad = torch.ones(1, max_length)\n            mask_tokens = torch.zeros(len(sequences), max_length)\n            for i, seq in enumerate(sequences):\n                end = lengths[i]\n                seq = torch.LongTensor(seq)\n                if len(seq) != 0:\n                    padded_seqs[i, :end] = seq[:end]\n                    mask_tokens[i, :end] = tmp_pad[0, :end]\n                    \n            return padded_seqs, mask_tokens\n        item_info = {}\n        for key in data_batch[0].keys():\n            item_info[key] = [d[key] for d in data_batch]\n        token_type_list = None\n        text_tokened, mask_tokens = merge(item_info['text_tokened'])\n        if not self.is_test:\n            token_type_list, _ = merge(item_info['token_type_list'])\n        # convert to contiguous and cuda\n        if USE_CUDA:\n            text_tokened = text_tokened.contiguous().cuda()\n            mask_tokens = mask_tokens.contiguous().cuda()\n        else:\n            text_tokened = text_tokened.contiguous()\n            mask_tokens = mask_tokens.contiguous()\n\n        if not self.is_test:\n            if USE_CUDA:\n                token_type_list = token_type_list.contiguous().cuda()\n\n            else:\n                token_type_list = token_type_list.contiguous()\n\n        data_info = {\"mask_tokens\": mask_tokens.to(torch.uint8)}\n        data_info['text'] = item_info['text']\n        data_info['spo_list'] = item_info['spo_list']\n        data_info['token_type_origin'] = item_info['token_type_origin']\n        for key in item_info.keys():\n            # try:\n            #     data_info[key] = locals()[key]\n            # except KeyError:\n            #     print('{} cannot be found in locals()'.format(key))\n            if key in locals():\n                data_info[key] = locals()[key]\n        \n        return data_info\n\nif __name__ == '__main__':\n    config = ConfigNer()\n    process = ModelDataPreparation(config)\n    train_loader, dev_loader, test_loader = process.get_train_dev_data('/kaggle/input/casrel-adr-data/adr-train.csv')\n    # train_loader, dev_loader, test_loader = process.get_train_dev_data('../data/train_data_small.json')\n   ","metadata":{"execution":{"iopub.status.busy":"2023-04-23T10:12:51.671070Z","iopub.execute_input":"2023-04-23T10:12:51.671388Z","iopub.status.idle":"2023-04-23T10:12:54.477145Z","shell.execute_reply.started":"2023-04-23T10:12:51.671361Z","shell.execute_reply":"2023-04-23T10:12:54.476064Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n# @Author  : xmh\n# @Time    : 2021/3/3 10:02\n# @File    : model_ner.py\n\n\"\"\"\nfile description:：\n\n\"\"\"\n!pip install -U -i https://pypi.tuna.tsinghua.edu.cn/simple pytorch-crf==0.7.0\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchcrf import CRF\n\nimport numpy as np\n\n\ndef setup_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nclass SeqLabel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        setup_seed(1)\n        \n        self.vocab_size = config.vocab_size\n        self.embedding_dim = config.embedding_dim\n        self.hidden_dim = config.hidden_dim_lstm\n        self.num_layers = config.num_layers\n        self.batch_size = config.batch_size\n        self.layer_size = config.layer_size  # self.hidden_dim, 之前这里没有改\n        self.num_token_type = config.num_token_type  # 实体类型的综述\n        self.config = config\n        \n        self.word_embedding = nn.Embedding(config.vocab_size, config.embedding_dim)\n        self.token_type_embedding = nn.Embedding(config.num_token_type, config.token_type_dim)\n        self.gru = nn.GRU(config.embedding_dim, config.hidden_dim_lstm, num_layers=config.num_layers, batch_first=True,\n                          bidirectional=True)\n        self.is_train = True\n        if USE_CUDA:\n            self.weights_rel = (torch.ones(self.config.num_relations) * 100).cuda()\n        else:\n            self.weights_rel = torch.ones(self.config.num_relations) * 100\n        self.weights_rel[0] = 1\n\n        self.V_ner = nn.Parameter(torch.rand((config.num_token_type, self.layer_size)))\n        self.U_ner = nn.Parameter(torch.rand((self.layer_size, 2 * self.hidden_dim)))\n        self.b_s_ner = nn.Parameter(torch.rand(self.layer_size))\n        self.b_c_ner = nn.Parameter(torch.rand(config.num_token_type))\n        \n        self.dropout_embedding_layer = torch.nn.Dropout(config.dropout_embedding)\n        self.dropout_ner_layer = torch.nn.Dropout(config.dropout_ner)\n        self.dropout_lstm_layer = torch.nn.Dropout(config.dropout_lstm)\n        self.crf_model = CRF(self.num_token_type,batch_first=True)\n        \n    def get_ner_score(self, output_lstm):\n        \n        res = torch.matmul(output_lstm, self.U_ner.transpose(-1, -2)) + self.b_s_ner # [seq_len, batch, self.layer_size]\n        res = torch.tanh(res)\n        # res = F.leaky_relu(res,  negative_slope=0.01)\n        if self.config.use_dropout:\n            res = self.dropout_ner_layer(res)\n            \n        ans = torch.matmul(res, self.V_ner.transpose(-1, -2)) + self.b_c_ner  # [seq_len, batch, num_token_type]\n        \n        return ans\n    \n    def forward(self, data_item, is_test=False):\n        # 因为不是多跳机制，所以hidden_init不能继承之前的最终隐含态\n        '''\n        \n        :param data_item: data_item = {'',}\n        :type data_item: dict\n        :return:\n        :rtype:\n        '''\n        # print(\"hello5\")\n        embeddings = self.word_embedding(data_item['text_tokened'].to(torch.int64))  # 要转化为int64\n        if self.config.use_dropout:\n            embeddings = self.dropout_embedding_layer(embeddings)\n        # if hidden_init is None:\n        # print(\"hello6\")\n        if USE_CUDA:\n            hidden_init = torch.randn(2*self.num_layers, self.batch_size, self.hidden_dim).cuda()\n        else:\n            hidden_init = torch.randn(2 * self.num_layers, self.batch_size, self.hidden_dim)\n        output_lstm, h_n =self.gru(embeddings, hidden_init)\n        # output_lstm [batch, seq_len, 2*hidden_dim]  h_n [2*num_layers, batch, hidden_dim]\n        # if self.config.use_dropout:\n        #     output_lstm = self.dropout_lstm_layer(output_lstm)  # 用了效果变差\n        ner_score = self.get_ner_score(output_lstm)\n        # 下面是使用CFR\n        if USE_CUDA:\n            self.crf_model = self.crf_model.cuda()\n        if not is_test:\n            log_likelihood = self.crf_model(ner_score, data_item['token_type_list'].to(torch.int64),\n                                       mask=data_item['mask_tokens'])\n            loss_ner = -log_likelihood\n            \n        pred_ner = self.crf_model.decode(ner_score)  # , mask=data_item['mask_tokens']\n        \n        if is_test:\n            return pred_ner\n        return loss_ner, pred_ner\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T10:12:54.479492Z","iopub.execute_input":"2023-04-23T10:12:54.480064Z","iopub.status.idle":"2023-04-23T10:13:05.605565Z","shell.execute_reply.started":"2023-04-23T10:12:54.480021Z","shell.execute_reply":"2023-04-23T10:13:05.604397Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\nCollecting pytorch-crf==0.7.0\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8b/1f/4b11a3547623953e33f4645e1672ef21dcd9d9b8e5a48337b270840ce9a0/pytorch_crf-0.7.0-py3-none-any.whl (10 kB)\nInstalling collected packages: pytorch-crf\nSuccessfully installed pytorch-crf-0.7.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n# @Author  : xmh\n# @Time    : 2021/3/3 17:28\n# @File    : trainer_ner.py\n\n\"\"\"\nfile description:：\n\n\"\"\"\n\n!pip install setuptools-scm\n!pip install seqeval\n!pip install neptune\n\nimport sys\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\n\nimport math\n\nfrom seqeval.metrics import f1_score\nfrom seqeval.metrics import precision_score\nfrom seqeval.metrics import accuracy_score\nfrom seqeval.metrics import recall_score\nfrom seqeval.metrics import classification_report\nimport neptune\n\n\nclass Trainer:\n    def __init__(self,\n                 model,\n                 config,\n                 train_dataset=None,\n                 dev_dataset=None,\n                 test_dataset=None,\n                 ):\n        self.model = model\n        self.train_dataset = train_dataset\n        self.dev_dataset = dev_dataset\n        self.test_dataset = test_dataset\n        self.config = config\n        \n        if USE_CUDA:\n            self.model = self.model.cuda()\n        # 初始优化器\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config.lr)\n        # 学习率调控\n        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5,\n                                                                   patience=8, min_lr=1e-5, verbose=True)\n        self.get_id2token_type()\n\n    def get_id2token_type(self):\n        self.id2token_type = {}\n        for i, token_type in enumerate(self.config.token_types):\n            self.id2token_type[i] = token_type\n        \n    def train(self):\n        print('STARTING TRAIN...')\n        f1_ner_total_best = 0.0\n        self.num_sample_total = len(self.train_dataset) * self.config.batch_size\n        for epoch in range(self.config.epochs):\n            print(\"Epoch: {}\".format(epoch))\n            pbar = tqdm(enumerate(self.train_dataset), total=len(self.train_dataset))\n            loss_ner_total, f1_ner_total = 0, 0\n            for i, data_item in pbar:\n                loss_ner, f1_ner, pred_ner = self.train_batch(data_item)\n                loss_ner_total += loss_ner\n                f1_ner_total += f1_ner\n                \n            if (epoch+1) % 1 == 0:\n                self.predict_sample()\n            loss_ner_train_ave = loss_ner_total/self.num_sample_total\n            print(\"train ner loss: {0}, f1 score: {1}\".format(loss_ner_train_ave,\n                                                        f1_ner_total/self.num_sample_total*self.config.batch_size))\n            # neptune.log_metric(\"train ner loss\", loss_ner_train_ave)\n            # pbar.set_description('TRAIN LOSS: {}'.format(loss_total/self.num_sample_total))\n            if (epoch+1) % 1 == 0:\n                self.evaluate()\n            if epoch > 8 and f1_ner_total >= f1_ner_total_best:\n                f1_ner_total_best = f1_ner_total\n                torch.save({\n                    'epoch': epoch+1, 'state_dict': self.model.state_dict(), 'f1_best': f1_ner_total,\n                    'optimizer': self.optimizer.state_dict(),\n                },\n                str(epoch) + 'm-' + 'f'+str(\"%.2f\"%f1_ner_total) + 'n'+\n                    str(\"%.2f\"%loss_ner_total) +'ccks2019_ner.pth'\n                )\n    \n    def train_batch(self, data_item):\n        self.optimizer.zero_grad()\n        loss_ner, pred_ner = self.model(data_item)\n        pred_token_type = self.restore_ner(pred_ner, data_item['mask_tokens'])\n        f1_ner = f1_score(data_item['token_type_origin'], pred_token_type)\n        loss_ner.backward()\n        self.optimizer.step()\n        \n        return loss_ner,f1_ner, pred_ner\n    \n    def restore_ner(self, pred_ner, mask_tokens):\n        pred_token_type = []\n        for i in range(len(pred_ner)):\n            list_tmp = []\n            for j in range(len(pred_ner[0])):\n                if mask_tokens[i, j] == 0:\n                    break\n                list_tmp.append(self.id2token_type[pred_ner[i][j]])\n            pred_token_type.append(list_tmp)\n            \n        return pred_token_type\n    \n    def evaluate(self):\n        print('STARTING EVALUATION...')\n        self.model.train(False)\n        pbar_dev = tqdm(enumerate(self.dev_dataset), total=len(self.dev_dataset))\n        \n        loss_total, loss_ner_total = 0, 0\n        for i, data_item in pbar_dev:\n            loss_ner, pred_ner = self.model(data_item)\n            loss_ner_total += loss_ner\n        \n        self.model.train(True)\n        print(\"eval ner loss: {0}\".format(loss_ner_total / (len(self.dev_dataset) * self.config.batch_size)))\n        # return loss_ner_total / (len(self.dev_dataset) * self.config.batch_size)\n    \n    def predict(self):\n        print('STARTING PREDICTING...')\n        self.model.train(False)\n        pbar = tqdm(enumerate(self.test_dataset), total=len(self.test_dataset))\n        for i, data_item in pbar:\n            pred_ner = self.model(data_item, is_test=True)\n        self.model.train(True)\n        token_pred = [[] for _ in range(len(pred_ner))]\n        for i in range(len(pred_ner)):\n            for item in pred_ner[i]:\n                token_pred[i].append(self.id2token_type[item])\n        return token_pred\n\n    def predict_sample(self):\n        print('STARTING TESTING...')\n        self.model.train(False)\n        pbar = tqdm(enumerate(self.test_dataset), total=len(self.test_dataset))\n\n        for i, data_item in pbar:\n            pred_ner = self.model(data_item, is_test=True)\n        data_item0 = data_item\n        pred_ner = pred_ner[0]\n        token_pred = []\n        for i in pred_ner:\n            token_pred.append(self.id2token_type[i])\n        print(\"token_pred: {}\".format(token_pred))\n        print(data_item0['text'][0])\n        print(data_item0['spo_list'][0])\n        self.model.train(True)\n        \n        \nif __name__ == '__main__':\n    # neptune.init(api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiNTM3OTQzY2ItMzRhNC00YjYzLWJhMTktMzI0NTk4NmM4NDc3In0=', project_qualified_name='mangopudding/EntityRelationExtraction')\n    # neptune.create_experiment('ner_train')\n    print(\"Run EntityRelationExtraction NER ...\")\n    config = ConfigNer()\n    model = SeqLabel(config)\n    data_processor = ModelDataPreparation(config)\n    train_loader, dev_loader, test_loader = data_processor.get_train_dev_data(\n        '/kaggle/input/casrel-adr-data/adr-train.csv',\n    '/kaggle/input/casrel-adr-data/test.csv',\n    '/kaggle/input/casrel-adr-data/test.csv')\n    # train_loader, dev_loader, test_loader = data_processor.get_train_dev_data('../data/train_data_small.json')\n    trainer = Trainer(model, config, train_loader, dev_loader, test_loader)\n    trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-04-23T10:13:05.607316Z","iopub.execute_input":"2023-04-23T10:13:05.607700Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting setuptools-scm\n  Downloading setuptools_scm-7.1.0-py3-none-any.whl (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from setuptools-scm) (23.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from setuptools-scm) (4.11.4)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from setuptools-scm) (59.8.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from setuptools-scm) (4.4.0)\nRequirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from setuptools-scm) (2.0.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->setuptools-scm) (3.11.0)\nInstalling collected packages: setuptools-scm\nSuccessfully installed setuptools-scm-7.1.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval) (1.21.6)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval) (1.0.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16179 sha256=e83855a83d119d0dbe205057f855129642fe8ccdcd690785a60027c3c4412e06\n  Stored in directory: /root/.cache/pip/wheels/b2/a1/b7/0d3b008d0c77cd57332d724b92cf7650b4185b493dc785f00a\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting neptune\n  Downloading neptune-1.1.1-py3-none-any.whl (442 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.6/442.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from neptune) (0.18.3)\nRequirement already satisfied: boto3>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from neptune) (1.26.100)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from neptune) (23.0)\nRequirement already satisfied: oauthlib>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from neptune) (3.2.2)\nRequirement already satisfied: requests-oauthlib>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from neptune) (1.3.1)\nCollecting swagger-spec-validator>=2.7.4\n  Downloading swagger_spec_validator-3.0.3-py2.py3-none-any.whl (27 kB)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.7/site-packages (from neptune) (8.1.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from neptune) (1.3.5)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from neptune) (1.16.0)\nRequirement already satisfied: PyJWT in /opt/conda/lib/python3.7/site-packages (from neptune) (2.6.0)\nCollecting bravado<12.0.0,>=11.0.0\n  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from neptune) (5.9.3)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from neptune) (4.11.4)\nRequirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.7/site-packages (from neptune) (2.28.2)\nRequirement already satisfied: urllib3 in /opt/conda/lib/python3.7/site-packages (from neptune) (1.26.14)\nRequirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /opt/conda/lib/python3.7/site-packages (from neptune) (1.4.2)\nRequirement already satisfied: Pillow>=1.1.6 in /opt/conda/lib/python3.7/site-packages (from neptune) (9.4.0)\nRequirement already satisfied: GitPython>=2.0.8 in /opt/conda/lib/python3.7/site-packages (from neptune) (3.1.30)\nCollecting botocore<1.30.0,>=1.29.100\n  Downloading botocore-1.29.118-py3-none-any.whl (10.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.0->neptune) (0.6.0)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.0->neptune) (1.0.1)\nRequirement already satisfied: msgpack in /opt/conda/lib/python3.7/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.0.4)\nRequirement already satisfied: simplejson in /opt/conda/lib/python3.7/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (3.18.4)\nCollecting monotonic\n  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (6.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (4.4.0)\nCollecting bravado-core>=5.16.1\n  Downloading bravado_core-5.17.1-py2.py3-none-any.whl (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (2.8.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=2.0.8->neptune) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->neptune) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->neptune) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->neptune) (2022.12.7)\nRequirement already satisfied: jsonschema in /opt/conda/lib/python3.7/site-packages (from swagger-spec-validator>=2.7.4->neptune) (4.17.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->neptune) (3.11.0)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->neptune) (2023.3)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas->neptune) (1.21.6)\nCollecting jsonref\n  Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune) (5.0.0)\nRequirement already satisfied: pkgutil-resolve-name>=1.3.10 in /opt/conda/lib/python3.7/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.3.10)\nRequirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (5.10.2)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.19.3)\nRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (22.2.0)\nCollecting rfc3339-validator\n  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\nCollecting rfc3987\n  Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\nCollecting uri-template\n  Downloading uri_template-1.2.0-py3-none-any.whl (10 kB)\nCollecting webcolors>=1.11\n  Downloading webcolors-1.13-py3-none-any.whl (14 kB)\nCollecting fqdn\n  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\nCollecting isoduration\n  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\nCollecting jsonpointer>1.13\n  Downloading jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\nRequirement already satisfied: cached-property>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from fqdn->jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.5.2)\nRequirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.2.3)\nInstalling collected packages: rfc3987, monotonic, webcolors, uri-template, rfc3339-validator, jsonref, jsonpointer, fqdn, botocore, swagger-spec-validator, isoduration, bravado-core, bravado, neptune\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.27.59\n    Uninstalling botocore-1.27.59:\n      Successfully uninstalled botocore-1.27.59\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.4.2 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.29.118 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed botocore-1.29.118 bravado-11.0.3 bravado-core-5.17.1 fqdn-1.5.1 isoduration-20.11.0 jsonpointer-2.3 jsonref-1.1.0 monotonic-1.6 neptune-1.1.1 rfc3339-validator-0.1.4 rfc3987-1.3.8 swagger-spec-validator-3.0.3 uri-template-1.2.0 webcolors-1.13\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRun EntityRelationExtraction NER ...\nSTARTING TRAIN...\nEpoch: 0\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/83 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torchcrf/__init__.py:261: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorCompare.cpp:413.)\n  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n100%|██████████| 83/83 [00:51<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"STARTING TESTING...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:02<00:00,  4.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"token_pred: ['B-adverse', 'I-adverse', 'I-adverse', 'I-adverse', 'I-adverse', 'I-adverse', 'I-adverse', 'I-adverse', 'I-adverse', 'I-adverse', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-drug', 'I-drug', 'I-drug', 'I-drug', 'I-drug', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-adverse', 'I-adverse', 'I-adverse', 'I-adverse', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-drug', 'I-drug', 'I-drug', 'I-drug', 'I-drug', 'I-drug', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-语言', 'I-学科专业', 'B-影视作品']\npneumonitis with pleural and pericardial effusion and neuropathy during amiodarone therapy.\n[]\ntrain ner loss: 546.4119873046875, f1 score: 2.4463335575805763e-05\nSTARTING EVALUATION...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:03<00:00,  3.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"eval ner loss: 126.29268646240234\nEpoch: 1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 83/83 [00:49<00:00,  1.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"STARTING TESTING...\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 6/10 [00:01<00:00,  5.18it/s]","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}