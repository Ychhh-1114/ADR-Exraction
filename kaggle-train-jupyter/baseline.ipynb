{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import  pandas as pd\n\n\n# def process_data(file_path):\n#     data = pd.read_csv(file_path)\n#     # print(data.columns)\n#     processed_data = []\n#\n#     for index, line in data.iterrows():\n#         dct = {\n#             \"text\": [],\n#             \"spo_list\": {\n#                 \"subject\": [],\n#                 \"object\": [],\n#                 \"predicate\": [],\n#             },\n#         }\n#         dct[\"text\"] = line[\"text\"]\n#         dct[\"spo_list\"][\"object\"] = line[\"effect\"]\n#         dct[\"spo_list\"][\"subject\"] = line[\"drug\"]\n#         dct[\"spo_list\"][\"predicate\"] = \"causes\"\n#         processed_data.append(dct)\n#\n#\n#     return processed_data\n#\n#\n#\ndef merge_data(data):\n    name_list = []\n    data_list = []\n\n    for each in data:\n        if each[\"text\"] not in name_list:\n            name_list.append(each[\"text\"])\n            data_list.append(each)\n        else:\n            index = name_list.index(each[\"text\"])\n            data_list[index][\"spo_list\"].append(each[\"spo_list\"][0])\n\n    return data_list\n\n\ndef process_data(file_path):\n    data = pd.read_csv(file_path)\n    # print(data.columns)\n    processed_data = []\n\n    for index, line in data.iterrows():\n        dct = {\n            \"text\": [],\n            \"spo_list\": [],\n        }\n        spo = {\n            \"subject\": \"\",\n            \"object\": \"\",\n            \"predicate\": \"\"\n        }\n        dct[\"text\"] = line[\"text\"]\n\n        spo[\"object\"] = line[\"effect\"]\n        spo[\"subject\"] = line[\"drug\"]\n        spo[\"predicate\"] = \"causes\"\n\n        dct[\"spo_list\"].append(spo)\n        processed_data.append(dct)\n\n    return merge_data(processed_data)\n\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-22T09:07:27.535980Z","iopub.execute_input":"2023-04-22T09:07:27.536494Z","iopub.status.idle":"2023-04-22T09:07:27.549247Z","shell.execute_reply.started":"2023-04-22T09:07:27.536454Z","shell.execute_reply":"2023-04-22T09:07:27.547950Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#! -*- coding:utf-8 -*-\n\nimport json\nfrom tqdm import tqdm\nimport codecs\n\n\n\nall_50_schemas = set()\n\nwith open('/kaggle/input/baseline/all_50_schemas') as f:\n    for l in tqdm(f):\n        a = json.loads(l)\n        all_50_schemas.add(a['predicate'])\n\nid2predicate = {i+1:j for i,j in enumerate(all_50_schemas)} # 0表示终止类别\npredicate2id = {j:i for i,j in id2predicate.items()}\n\nwith codecs.open('/kaggle/working/all_50_schemas_me.json', 'w', encoding='utf-8') as f:\n    json.dump([id2predicate, predicate2id], f, indent=4, ensure_ascii=False)\n\n\nchars = {}\nmin_count = 2\n\n\ntrain_data = []\n\ntrain_data_pro = process_data(\"/kaggle/input/casrel-adr-data/adr-train.csv\")\nfor each in train_data_pro:\n    train_data.append(\n        {\n            'text': each['text'],\n            'spo_list': [(i['subject'], i['predicate'], i['object']) for i in each['spo_list']]\n        }\n    )\n    for c in each['text']:\n        chars[c] = chars.get(c, 0) + 1\n\n\n\nwith codecs.open('/kaggle/working/train_data_me.json', 'w', encoding='utf-8') as f:\n    json.dump(train_data, f, indent=4, ensure_ascii=False)\n\n\ndev_data = []\n\ntrain_data_pro = process_data(\"/kaggle/input/casrel-adr-data/adr-train.csv\")\nfor each in train_data_pro:\n    dev_data.append(\n        {\n            'text': each['text'],\n            'spo_list': [(i['subject'], i['predicate'], i['object']) for i in each['spo_list']]\n        }\n    )\n    for c in each['text']:\n        chars[c] = chars.get(c, 0) + 1\n\nwith codecs.open('/kaggle/working/dev_data_me.json', 'w', encoding='utf-8') as f:\n    json.dump(dev_data, f, indent=4, ensure_ascii=False)\n\n\nwith codecs.open('/kaggle/working/all_chars_me.json', 'w', encoding='utf-8') as f:\n\n    chars = {i:j for i,j in chars.items() if j >= min_count}\n    id2char = {i+2:j for i,j in enumerate(chars)} # padding: 0, unk: 1\n    char2id = {j:i for i,j in id2char.items()}\n    json.dump([id2char, char2id], f, indent=4, ensure_ascii=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:07:27.981055Z","iopub.execute_input":"2023-04-22T09:07:27.981933Z","iopub.status.idle":"2023-04-22T09:07:29.307649Z","shell.execute_reply.started":"2023-04-22T09:07:27.981875Z","shell.execute_reply":"2023-04-22T09:07:29.306268Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"51it [00:00, 71613.49it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport numpy as np\n# import matplotlib.pyplot as plt\nfrom torch.autograd import Variable\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef seq_max_pool(x):\n    \"\"\"seq是[None, seq_len, s_size]的格式，\n    mask是[None, seq_len, 1]的格式，先除去mask部分，\n    然后再做maxpooling。\n    \"\"\"\n    seq, mask = x\n    seq = seq - (1 - mask) * 1e10\n    return torch.max(seq, 1)\n\n\ndef seq_and_vec(x):\n    \"\"\"seq是[None, seq_len, s_size]的格式，\n    vec是[None, v_size]的格式，将vec重复seq_len次，拼到seq上，\n    得到[None, seq_len, s_size+v_size]的向量。\n    \"\"\"\n    seq, vec = x\n    vec = torch.unsqueeze(vec, 1)\n\n    vec = torch.zeros_like(seq[:, :, :1]) + vec\n    return torch.cat([seq, vec], 2)\n\n\ndef seq_gather(x):\n    \"\"\"seq是[None, seq_len, s_size]的格式，\n    idxs是[None, 1]的格式，在seq的第i个序列中选出第idxs[i]个向量，\n    最终输出[None, s_size]的向量。\n    \"\"\"\n    seq, idxs = x\n    batch_idxs = torch.arange(0, seq.size(0)).to(device)\n\n    batch_idxs = torch.unsqueeze(batch_idxs, 1)\n\n    idxs = torch.cat([batch_idxs, idxs], 1)\n\n    res = []\n    for i in range(idxs.size(0)):\n        vec = seq[idxs[i][0], idxs[i][1], :]\n        res.append(torch.unsqueeze(vec, 0))\n\n    res = torch.cat(res)\n    return res\n\n\nclass s_model(nn.Module):\n    def __init__(self, word_dict_length, word_emb_size, lstm_hidden_size):\n        super(s_model, self).__init__()\n\n        self.embeds = nn.Embedding(word_dict_length, word_emb_size).to(device)\n        self.fc1_dropout = nn.Sequential(\n            nn.Dropout(0.25).to(device),  # drop 20% of the neuron\n        ).to(device)\n\n        self.lstm1 = nn.LSTM(\n            input_size=word_emb_size,\n            hidden_size=int(word_emb_size / 2),\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True\n        ).to(device)\n\n        self.lstm2 = nn.LSTM(\n            input_size=word_emb_size,\n            hidden_size=int(word_emb_size / 2),\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True\n        ).to(device)\n\n        self.conv1 = nn.Sequential(\n            nn.Conv1d(\n                in_channels=word_emb_size * 2,  # 输入的深度\n                out_channels=word_emb_size,  # filter 的个数，输出的高度\n                kernel_size=3,  # filter的长与宽\n                stride=1,  # 每隔多少步跳一下\n                padding=1,  # 周围围上一圈 if stride= 1, pading=(kernel_size-1)/2\n            ).to(device),\n            nn.ReLU().to(device),\n        ).to(device)\n        self.fc_ps1 = nn.Sequential(\n            nn.Linear(word_emb_size, 1),\n        ).to(device)\n\n        self.fc_ps2 = nn.Sequential(\n            nn.Linear(word_emb_size, 1),\n        ).to(device)\n\n    def forward(self, t):\n        mask = torch.gt(torch.unsqueeze(t, 2), 0).type(torch.FloatTensor)  # (batch_size,sent_len,1)\n        mask.requires_grad = False\n\n        outs = self.embeds(t)\n\n        t = outs\n        t = self.fc1_dropout(t)\n\n        t = t.mul(mask)  # (batch_size,sent_len,char_size)\n\n        t, (h_n, c_n) = self.lstm1(t, None)\n        t, (h_n, c_n) = self.lstm2(t, None)\n\n        t_max, t_max_index = seq_max_pool([t, mask])\n\n        t_dim = list(t.size())[-1]\n        h = seq_and_vec([t, t_max])\n\n        h = h.permute(0, 2, 1)\n\n        h = self.conv1(h)\n\n        h = h.permute(0, 2, 1)\n\n        ps1 = self.fc_ps1(h)\n        ps2 = self.fc_ps2(h)\n\n        return [ps1.to(device), ps2.to(device), t.to(device), t_max.to(device), mask.to(device)]\n\n\nclass po_model(nn.Module):\n    def __init__(self, word_dict_length, word_emb_size, lstm_hidden_size, num_classes):\n        super(po_model, self).__init__()\n\n        self.conv1 = nn.Sequential(\n            nn.Conv1d(\n                in_channels=word_emb_size * 4,  # 输入的深度\n                out_channels=word_emb_size,  # filter 的个数，输出的高度\n                kernel_size=3,  # filter的长与宽\n                stride=1,  # 每隔多少步跳一下\n                padding=1,  # 周围围上一圈 if stride= 1, pading=(kernel_size-1)/2\n            ).to(device),\n            nn.ReLU().to(device),\n        ).to(device)\n\n        self.fc_ps1 = nn.Sequential(\n            nn.Linear(word_emb_size, num_classes + 1).to(device),\n            # nn.Softmax(),\n        ).to(device)\n\n        self.fc_ps2 = nn.Sequential(\n            nn.Linear(word_emb_size, num_classes + 1).to(device),\n            # nn.Softmax(),\n        ).to(device)\n\n    def forward(self, t, t_max, k1, k2):\n        k1 = seq_gather([t, k1])\n\n        k2 = seq_gather([t, k2])\n\n        k = torch.cat([k1, k2], 1)\n        h = seq_and_vec([t, t_max])\n        h = seq_and_vec([h, k])\n        h = h.permute(0, 2, 1)\n        h = self.conv1(h)\n        h = h.permute(0, 2, 1)\n\n        po1 = self.fc_ps1(h)\n        po2 = self.fc_ps2(h)\n\n        return [po1.to(device), po2.to(device)]\n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:07:29.312651Z","iopub.execute_input":"2023-04-22T09:07:29.313019Z","iopub.status.idle":"2023-04-22T09:07:31.491118Z","shell.execute_reply.started":"2023-04-22T09:07:29.312985Z","shell.execute_reply":"2023-04-22T09:07:31.489870Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#! -*- coding:utf-8 -*-\n\nimport json\nimport numpy as np\nfrom random import choice\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.autograd import Variable\n# import data_prepare\nimport os\nimport torch.utils.data as Data\nimport torch.nn.functional as F\n\nimport time\n\n\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\n\ntorch.backends.cudnn.benchmark = True\n\nCHAR_SIZE = 1500\nSENT_LENGTH = 4\nHIDDEN_SIZE = 64\nEPOCH_NUM = 100\n\nBATCH_SIZE = 64\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef get_now_time():\n    a = time.time()\n    return time.ctime(a)\n\n\ndef seq_padding(X):\n    L = [len(x) for x in X]\n    ML = max(L)\n    # print(\"ML\",ML)\n    return [x + [0] * (ML - len(x)) for x in X]\n\n\ndef seq_padding_vec(X):\n    L = [len(x) for x in X]\n    ML = max(L)\n    # print(\"ML\",ML)\n    return [x + [[1, 0]] * (ML - len(x)) for x in X]\n\n\ntrain_path = \"/kaggle/input/casrel-adr-data/adr-train.csv\"\ndev_path  = \"/kaggle/input/casrel-adr-data/adr-train.csv\"\nid2predicate, predicate2id = json.load(open('/kaggle/working/all_50_schemas_me.json'))\nid2predicate = {int(i): j for i, j in id2predicate.items()}\nid2char, char2id = json.load(open('/kaggle/working/all_chars_me.json'))\nnum_classes = len(id2predicate)\n\n\nclass data_generator:\n    def __init__(self, path, batch_size=64):\n        self.data = process_data(path)\n        self.batch_size = batch_size\n        self.steps = len(self.data) // self.batch_size\n        if len(self.data) % self.batch_size != 0:\n            self.steps += 1\n\n    def __len__(self):\n        return self.steps\n\n    def pro_res(self):\n        idxs = list(range(len(self.data)))\n        # print(idxs)\n        np.random.shuffle(idxs)\n        T, S1, S2, K1, K2, O1, O2, = [], [], [], [], [], [], []\n        for i in idxs:\n            d = self.data[i]\n            text = d['text']\n            items = {}\n            for sp in d['spo_list']:\n                subjectid = text.find(sp[\"subject\"])\n                objectid = text.find(sp[\"object\"])\n                if subjectid != -1 and objectid != -1:\n                    key = (subjectid, subjectid + len(sp[\"subject\"]))\n                    if key not in items:\n                        items[key] = []\n                    items[key].append((objectid, objectid + len(sp[\"object\"]), predicate2id[sp[\"predicate\"]]))\n            if items:\n                T.append([char2id.get(c, 1) for c in text])  # 1是unk，0是padding\n                # s1, s2 = [[1,0]] * len(text), [[1,0]] * len(text)\n                s1, s2 = [0] * len(text), [0] * len(text)\n                for j in items:\n                    # s1[j[0]] = [0,1]\n                    # s2[j[1]-1] = [0,1]\n                    s1[j[0]] = 1\n                    s2[j[1] - 1] = 1\n                # print(items.keys())\n                k1, k2 = choice(list(items.keys()))\n                o1, o2 = [0] * len(text), [0] * len(text)  # 0是unk类（共49+1个类）\n                for j in items[(k1, k2)]:\n                    o1[j[0]] = j[2]\n                    o2[j[1] - 1] = j[2]\n                S1.append(s1)\n                S2.append(s2)\n                K1.append([k1])\n                K2.append([k2 - 1])\n                O1.append(o1)\n                O2.append(o2)\n\n        T = np.array(seq_padding(T))\n        S1 = np.array(seq_padding(S1))\n        S2 = np.array(seq_padding(S2))\n        O1 = np.array(seq_padding(O1))\n        O2 = np.array(seq_padding(O2))\n        K1, K2 = np.array(K1), np.array(K2)\n        return [T, S1, S2, K1, K2, O1, O2]\n\n\nclass myDataset(Data.Dataset):\n    \"\"\"\n        下载数据、初始化数据，都可以在这里完成\n    \"\"\"\n\n    def __init__(self, _T, _S1, _S2, _K1, _K2, _O1, _O2):\n        # xy = np.loadtxt('../dataSet/diabetes.csv.gz', delimiter=',', dtype=np.float32) # 使用numpy读取数据\n        self.x_data = _T\n        self.y1_data = _S1\n        self.y2_data = _S2\n        self.k1_data = _K1\n        self.k2_data = _K2\n        self.o1_data = _O1\n        self.o2_data = _O2\n        self.len = len(self.x_data)\n\n    def __getitem__(self, index):\n        return self.x_data[index], self.y1_data[index], self.y2_data[index], self.k1_data[index], self.k2_data[index], \\\n               self.o1_data[index], self.o2_data[index]\n\n    def __len__(self):\n        return self.len\n\n\ndef collate_fn(data):\n    t = np.array([item[0] for item in data], np.int32)\n    s1 = np.array([item[1] for item in data], np.int32)\n    s2 = np.array([item[2] for item in data], np.int32)\n    k1 = np.array([item[3] for item in data], np.int32)\n\n    k2 = np.array([item[4] for item in data], np.int32)\n    o1 = np.array([item[5] for item in data], np.int32)\n    o2 = np.array([item[6] for item in data], np.int32)\n    return {\n        'T': torch.LongTensor(t),  # targets_i\n        'S1': torch.FloatTensor(s1),\n        'S2': torch.FloatTensor(s2),\n        'K1': torch.LongTensor(k1),\n        'K2': torch.LongTensor(k2),\n        'O1': torch.LongTensor(o1),\n        'O2': torch.LongTensor(o2),\n    }\n\n\ndg = data_generator(train_path)\ndev_data = process_data(dev_path)\nT, S1, S2, K1, K2, O1, O2 = dg.pro_res()\n# print(\"len\",len(T))\n\ntorch_dataset = myDataset(T, S1, S2, K1, K2, O1, O2)\nloader = Data.DataLoader(\n    dataset=torch_dataset,  # torch TensorDataset format\n    batch_size=BATCH_SIZE,  # mini batch size\n    shuffle=True,  # random shuffle for training\n    num_workers=0,\n    collate_fn=collate_fn,  # subprocesses for loading data\n)\n\n# print(\"len\",len(id2char))\ns_m = s_model(len(char2id) + 2, CHAR_SIZE, HIDDEN_SIZE).to(device)\npo_m = po_model(len(char2id) + 2, CHAR_SIZE, HIDDEN_SIZE, 49).to(device)\nparams = list(s_m.parameters())\n\nparams += list(po_m.parameters())\noptimizer = torch.optim.Adam(params, lr=0.001)\n\nloss = torch.nn.CrossEntropyLoss().to(device)\nb_loss = torch.nn.BCEWithLogitsLoss().to(device)\n\n\ndef extract_items(text_in):\n    R = []\n    _s = [char2id.get(c, 1) for c in text_in]\n    _s = np.array([_s])\n    _k1, _k2, t, t_max, mask = s_m(torch.LongTensor(_s).to(device))\n    _k1, _k2 = _k1[0, :, 0], _k2[0, :, 0]\n    _kk1s = []\n    for i, _kk1 in enumerate(_k1):\n        if _kk1 > 0.5:\n            _subject = ''\n            for j, _kk2 in enumerate(_k2[i:]):\n                if _kk2 > 0.5:\n                    _subject = text_in[i: i + j + 1]\n                    break\n            if _subject:\n                _k1, _k2 = torch.LongTensor([[i]]), torch.LongTensor([[i + j]])  # np.array([i]), np.array([i+j])\n                _o1, _o2 = po_m(t.to(device), t_max.to(device), _k1.to(device), _k2.to(device))\n                _o1, _o2 = _o1.cpu().data.numpy(), _o2.cpu().data.numpy()\n\n                _o1, _o2 = np.argmax(_o1[0], 1), np.argmax(_o2[0], 1)\n\n                for i, _oo1 in enumerate(_o1):\n                    if _oo1 > 0:\n                        for j, _oo2 in enumerate(_o2[i:]):\n                            if _oo2 == _oo1:\n                                _object = text_in[i: i + j + 1]\n                                _predicate = id2predicate[_oo1]\n                                # print((_subject, _predicate, _object))\n                                R.append((_subject, _predicate, _object))\n                                break\n        _kk1s.append(_kk1.data.cpu().numpy())\n    _kk1s = np.array(_kk1s)\n    return list(set(R))\n\n\ndef evaluate():\n    A, B, C = 1e-10, 1e-10, 1e-10\n    cnt = 0\n    for d in tqdm(iter(dev_data)):\n        R = set(extract_items(d['text']))\n        T = set([tuple(i) for i in d['spo_list']])\n        A += len(R & T)\n        B += len(R)\n        C += len(T)\n        # if cnt % 1000 == 0:\n        #     print('iter: %d f1: %.4f, precision: %.4f, recall: %.4f\\n' % (cnt, 2 * A / (B + C), A / B, A / C))\n        cnt += 1\n    return 2 * A / (B + C), A / B, A / C\n\n\nbest_f1 = 0\nbest_epoch = 0\n\nfor i in range(EPOCH_NUM):\n    for step, loader_res in tqdm(iter(enumerate(loader))):\n        # print(get_now_time())\n        t_s = loader_res[\"T\"].to(device)\n        k1 = loader_res[\"K1\"].to(device)\n        k2 = loader_res[\"K2\"].to(device)\n        s1 = loader_res[\"S1\"].to(device)\n        s2 = loader_res[\"S2\"].to(device)\n        o1 = loader_res[\"O1\"].to(device)\n        o2 = loader_res[\"O2\"].to(device)\n\n        ps_1, ps_2, t, t_max, mask = s_m(t_s)\n\n        t, t_max, k1, k2 = t.to(device), t_max.to(device), k1.to(device), k2.to(device)\n        po_1, po_2 = po_m(t, t_max, k1, k2)\n\n        ps_1 = ps_1.to(device)\n        ps_2 = ps_2.to(device)\n        po_1 = po_1.to(device)\n        po_2 = po_2.to(device)\n\n        s1 = torch.unsqueeze(s1, 2)\n        s2 = torch.unsqueeze(s2, 2)\n\n        s1_loss = b_loss(ps_1, s1)\n        s1_loss = torch.sum(s1_loss.mul(mask)) / torch.sum(mask)\n        s2_loss = b_loss(ps_2, s2)\n        s2_loss = torch.sum(s2_loss.mul(mask)) / torch.sum(mask)\n\n        po_1 = po_1.permute(0, 2, 1)\n        po_2 = po_2.permute(0, 2, 1)\n\n        o1_loss = loss(po_1, o1)\n        o1_loss = torch.sum(o1_loss.mul(mask[:, :, 0])) / torch.sum(mask)\n        o2_loss = loss(po_2, o2)\n        o2_loss = torch.sum(o2_loss.mul(mask[:, :, 0])) / torch.sum(mask)\n\n        loss_sum = 2.5 * (s1_loss + s2_loss) + (o1_loss + o2_loss)\n\n        # if step % 500 == 0:\n        # \ttorch.save(s_m, 'models_real/s_'+str(step)+\"epoch_\"+str(i)+'.pkl')\n        # \ttorch.save(po_m, 'models_real/po_'+str(step)+\"epoch_\"+str(i)+'.pkl')\n\n        optimizer.zero_grad()\n\n        loss_sum.backward()\n        optimizer.step()\n\n    torch.save(s_m, 'models_real/s_' + str(i) + '.pkl')\n    torch.save(po_m, 'models_real/po_' + str(i) + '.pkl')\n    f1, precision, recall = evaluate()\n\n    print(\"epoch:\", i, \"loss:\", loss_sum.data)\n\n    if f1 >= best_f1:\n        best_f1 = f1\n        best_epoch = i\n\n    print('f1: %.4f, precision: %.4f, recall: %.4f, bestf1: %.4f, bestepoch: %d \\n ' % (\n    f1, precision, recall, best_f1, best_epoch))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:08:57.937604Z","iopub.execute_input":"2023-04-22T09:08:57.938052Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"}]}]}