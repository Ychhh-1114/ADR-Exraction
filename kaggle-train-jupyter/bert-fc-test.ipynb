{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n'''\n@Description:       : dataloader and process data\n@Author             : Kevinpro\n@version            : 1.0\n'''\nimport json\nfrom transformers import BertTokenizer\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.autograd as autograd\nimport torch.nn.functional\nfrom torch.utils.data import Dataset, DataLoader\nimport random\n\nimport numpy as np\n\nfrom process import process_data\n\n\ndef setup_seed(seed):\n     torch.manual_seed(seed)\n     torch.cuda.manual_seed_all(seed)\n     np.random.seed(seed)\n     random.seed(seed)\n\nsetup_seed(44)\ndef prepare_data():\n    print(\"---Regenerate Data---\")\n    with open(\"train_data.json\", 'r', encoding='utf-8') as load_f:\n        info=[]\n        import random\n        for line in load_f.readlines():\n            dic = json.loads(line)\n            for j in dic['spo_list']:\n                single_data={}\n                single_data['rel']=j[\"predicate\"]\n                single_data['ent1']=j[\"object\"]\n                single_data['ent2'] = j[\"subject\"]\n                single_data['text']=dic['text']\n                info.append(single_data)\n        sub_train = info\n    with open(\"train.json\", \"w\",encoding='utf-8') as dump_f:\n        for i in sub_train:\n            a = json.dumps(i, ensure_ascii=False)\n            dump_f.write(a)\n            dump_f.write(\"\\n\")\n    \n    with open(\"dev_data.json\", 'r', encoding='utf-8') as load_f:\n        info=[]\n        import random\n        for line in load_f.readlines():\n            dic = json.loads(line)\n            for j in dic['spo_list']:\n                single_data={}\n                single_data['rel']=j[\"predicate\"]\n                single_data['ent1']=j[\"object\"]\n                single_data['ent2'] = j[\"subject\"]\n                single_data['text']=dic['text']\n                info.append(single_data)\n            \n        sub_train = info\n    with open(\"dev.json\", \"w\",encoding='utf-8') as dump_f:\n        for i in sub_train:\n            a = json.dumps(i, ensure_ascii=False)\n            dump_f.write(a)\n            dump_f.write(\"\\n\")\n\n#prepare_data()\n\n\n# def map_id_rel():\n#     rel = [\"UNK\"]\n#     with open(\"train.json\", 'r', encoding='utf-8') as load_f:\n#         for line in load_f.readlines():\n#             dic = json.loads(line)\n#             if dic['rel'] not in rel:\n#                 rel.append(dic['rel'])\n#     id2rel={}\n#     rel2id={}\n#     for i in range(len(rel)):\n#         id2rel[i]=rel[i]\n#         rel2id[rel[i]]=i\n#     return rel2id,id2rel\n\ndef map_id_rel():\n    id2rel={0: 'UNK', 1: '主演', 2: '歌手', 3: '简称', 4: '总部地点', 5: '导演', 6: '出生地', 7: '目', 8: '出生日期', 9: '占地面积', 10: '上映时间', 11: '出版社', 12: '作者', 13: '号', 14: '父亲', 15: '毕业院校', 16: '成立日期', 17: '改编自', 18: '主持人', 19: '所属专辑', 20: '连载网站', 21: '作词', 22: '作曲', 23: '创始人', 24: '丈夫', 25: '妻子', 26: '朝代', 27: '民族', 28: '国籍', 29: '身高', 30: '出品公司', 31: '母亲', 32: '编剧', 33: '首都', 34: '面积', 35: '祖籍', 36: '嘉宾', 37: '字', 38: '海拔', 39: '注册资本', 40: '制片人', 41: '董事长', 42: '所在城市', 43: '气候', 44: '人口数量', 45: '邮政编码', 46: '主角', 47: '官方语言', 48: '修业年限',49:\"causes\"}\n    rel2id={}\n    for i in id2rel:\n        rel2id[id2rel[i]]=i\n    return rel2id,id2rel\n\n\ndef load_dev():\n    rel2id,id2rel=map_id_rel()\n    max_length=512\n    tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n    train_data = {}\n    train_data['label'] = []\n    train_data['mask'] = []\n    train_data['text'] = []\n\n    process_data_ = process_data(\"/kaggle/input/casrel-adr-data/test.csv\")\n    for each in process_data_:\n        # print(each)\n        sent = each['ent1'] + each['ent2'] + each['text']\n        if each['rel'] not in rel2id:\n            train_data['label'].append(0)\n        else:\n            train_data['label'].append(rel2id[each['rel']])\n\n        indexed_tokens = tokenizer.encode(sent, add_special_tokens=True)\n        avai_len = len(indexed_tokens)\n        while len(indexed_tokens) < max_length:\n            indexed_tokens.append(0)  # 0 is id for [PAD]\n        indexed_tokens = indexed_tokens[: max_length]\n        indexed_tokens = torch.tensor(indexed_tokens).long().unsqueeze(0)  # (1, L)\n\n        # Attention mask\n        att_mask = torch.zeros(indexed_tokens.size()).long()  # (1, L)\n        att_mask[0, :avai_len] = 1\n        train_data['text'].append(indexed_tokens)\n        train_data['mask'].append(att_mask)\n        # print(train_data)\n    return train_data\n\n\n\nif __name__ == '__main__':\n    load_train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport time\nimport copy\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.autograd as autograd\nimport torch.nn.functional\nfrom torch.utils.data import Dataset, DataLoader\n# from torchvision import transforms\nimport warnings\nimport torch\nimport time\nimport argparse\n\nimport random\ndef setup_seed(seed):\n     torch.manual_seed(seed)\n     torch.cuda.manual_seed_all(seed)\n     np.random.seed(seed)\n     random.seed(seed)\n\nsetup_seed(44)\n\nfrom transformers import BertModel\n\nfrom loader import map_id_rel, load_dev\n\nrel2id, id2rel = map_id_rel()\n\nprint(len(rel2id))\nprint(id2rel)\n\nUSE_CUDA = torch.cuda.is_available()\n\ndef get_train_args():\n    labels_num=len(rel2id)\n    parser=argparse.ArgumentParser()\n    parser.add_argument('--batch_size',type=int,default=1,help = '每批数据的数量')\n    parser.add_argument('--nepoch',type=int,default=30,help = '训练的轮次')\n    parser.add_argument('--lr',type=float,default=0.001,help = '学习率')\n    parser.add_argument('--gpu',type=bool,default=True,help = '是否使用gpu')\n    parser.add_argument('--num_workers',type=int,default=2,help='dataloader使用的线程数量')\n    parser.add_argument('--num_labels',type=int,default=len(id2rel),help='分类类数')\n    parser.add_argument('--data_path',type=str,default='./data',help='数据路径')\n    opt=parser.parse_args()\n    print(opt)\n    return opt\n\n\n\n\n# def test(net,text_list,ent1_list,ent2_list,result):\n#     net.eval()\n#     max_length=128\n#\n#     net=torch.load('bert-fc.pth')\n#     rel_list=[]\n#     with torch.no_grad():\n#         for text,ent1,ent2,label in zip(text_list,ent1_list,ent2_list,result):\n#             sent = ent1 + ent2+ text\n#             tokenizer = BertTokenizer.from_pretrained('./bert-model/biobert-base-cased-v1.2')\n#             indexed_tokens = tokenizer.encode(sent, add_special_tokens=True)\n#             avai_len = len(indexed_tokens)\n#             while len(indexed_tokens) < max_length:\n#                 indexed_tokens.append(0)  # 0 is id for [PAD]\n#             indexed_tokens = indexed_tokens[: max_length]\n#             indexed_tokens = torch.tensor(indexed_tokens).long().unsqueeze(0)  # (1, L)\n#\n#             # Attention mask\n#             att_mask = torch.zeros(indexed_tokens.size()).long()  # (1, L)\n#             att_mask[0, :avai_len] = 1\n#             if USE_CUDA:\n#                 indexed_tokens = indexed_tokens.cuda()\n#                 att_mask = att_mask.cuda()\n#\n#             if USE_CUDA:\n#                 indexed_tokens=indexed_tokens.cuda()\n#                 att_mask=att_mask.cuda()\n#             outputs = net(indexed_tokens, attention_mask=att_mask)\n#             # print(y)\n#             logits = outputs[0]\n#             _, predicted = torch.max(logits.data, 1)\n#             result=predicted.cpu().numpy().tolist()[0]\n#             print(\"Source Text: \",text)\n#             print(\"Entity1: \",ent1,\" Entity2: \",ent2,\" Predict Relation: \",id2rel[result],\" True Relation: \",label)\n#             print('\\n')\n#             rel_list.append(id2rel[result])\n#     return rel_list\n# opt = get_train_args()\n#\n# model=get_model(opt)\n\n\nfrom random import choice\n\ntext_list=[]\nent1=[]\nent2=[]\nresult=[]\n\ndata=load_dev()\ndev_text=data['text']\ndev_mask=data['mask']\ndev_label=data['label']\n\ndev_text = [ t.numpy() for t in dev_text]\ndev_mask = [ t.numpy() for t in dev_mask]\n\ndev_text=torch.tensor(dev_text)\ndev_mask=torch.tensor(dev_mask)\ndev_label=torch.tensor(dev_label)\n\nbatch_size = 20\ndev_dataset = torch.utils.data.TensorDataset(dev_text,dev_mask,dev_label)\n\ndev_iter = torch.utils.data.DataLoader(dev_dataset, batch_size, shuffle=True)\n\ndef get_model():\n    labels_num=len(rel2id)\n    from model import BERT_Classifier\n    model = BERT_Classifier(labels_num)\n    return model\n\n\nclass BERT_Classifier(nn.Module):\n    def __init__(self,label_num):\n        super().__init__()\n        self.encoder = BertModel.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n        for name, param in self.encoder.named_parameters():\n            param.requires_grad = False\n\n        self.dropout = nn.Dropout(0.1,inplace=False)\n        self.fc = nn.Linear(768, label_num)\n        self.criterion = nn.CrossEntropyLoss()\n    def forward(self, x, attention_mask ,label=None):\n        x = self.encoder(x, attention_mask=attention_mask)[0]\n        x = x[:, 0, :]\n        x = self.dropout(x)\n        x = self.fc(x)\n        if label == None:\n            return None,x\n        else:\n            return self.criterion(x,label),x\n\n\nnet = torch.load('bert-fc.pth')\ncorrect = 0\ntotal=0\niter = 0\n\nfor text, mask, y in dev_iter:\n    iter += 1\n    # print(type(y))\n    # print(y)\n    if text.size(0) != batch_size:\n        break\n    text = text.reshape(batch_size, -1)\n    mask = mask.reshape(batch_size, -1)\n    if USE_CUDA:\n        text = text.cuda()\n        mask = mask.cuda()\n        y = y.cuda()\n    # print(text.shape)\n    loss, logits = net(text, mask, y)\n    # print(y)\n    # print(loss.shape)\n    # print(\"predicted\",predicted)\n    # print(\"answer\", y)\n    # print(outputs[1].shape)\n    # print(output)\n    # print(outputs[1])\n    _, predicted = torch.max(logits.data, 1)\n    total += text.size(0)\n    correct += predicted.data.eq(y.data).cpu().sum()\n\n    if iter % 5 == 0:\n        print(\"iter: \",iter)\n\nloss = loss.detach().cpu()\nprint( \" loss: \", loss.mean().numpy().tolist(), \"right\", correct.cpu().numpy().tolist(),\n      \"total\", total, \"Acc:\", correct.cpu().numpy().tolist() / total)\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}